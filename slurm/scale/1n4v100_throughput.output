WORLD_SIZE=4
MASTER_ADDR=udc-aj38-35
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 217.63 | loss   nan | ppl      nan
| batch     2 | wps 1595.31 | loss   nan | ppl      nan
| batch     3 | wps 1593.89 | loss   nan | ppl      nan
| batch     4 | wps 1585.67 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 11.66s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 1486.99.
Elapsed_time(s) is 11.66.
Peak allocated bytes on cuda:0: 8.662100GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=2, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 367.86 | loss   nan | ppl      nan
| batch     2 | wps 2400.56 | loss   nan | ppl      nan
| batch     3 | wps 2400.95 | loss   nan | ppl      nan
| batch     4 | wps 2397.32 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 14.03s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 2364.08.
Elapsed_time(s) is 14.03.
Peak allocated bytes on cuda:0: 14.119191GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=3, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 613.38 | loss   nan | ppl      nan
| batch     2 | wps 2920.08 | loss   nan | ppl      nan
| batch     3 | wps 2922.05 | loss   nan | ppl      nan
| batch     4 | wps 2922.04 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 13.49s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 2892.97.
Elapsed_time(s) is 13.49.
Peak allocated bytes on cuda:0: 19.542492GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=4, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 722.83 | loss   nan | ppl      nan
| batch     2 | wps 3165.68 | loss   nan | ppl      nan
| batch     3 | wps 3156.90 | loss   nan | ppl      nan
| batch     4 | wps 3167.51 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 15.56s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 3115.23.
Elapsed_time(s) is 15.56.
Peak allocated bytes on cuda:0: 24.890339GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 365.61 | loss   nan | ppl      nan
| batch     2 | wps 1016.99 | loss   nan | ppl      nan
| batch     3 | wps 1058.13 | loss   nan | ppl      nan
| batch     4 | wps 1060.92 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 34.51s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 1045.82.
Elapsed_time(s) is 34.51.
Peak allocated bytes on cuda:0: 25.575084GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=2, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running RTP-out-of-place benchmark with args: Namespace(batch_size=3, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
