WORLD_SIZE=32
MASTER_ADDR=udc-aj33-9
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 657.25 | loss   nan | ppl      nan
| batch     2 | wps 911.01 | loss   nan | ppl      nan
| batch     3 | wps 913.58 | loss   nan | ppl      nan
| batch     4 | wps 916.09 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time:  6.86s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 899.26.
Elapsed_time(s) is 6.86.
Peak allocated bytes on cuda:0: 18.644805GB
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 259.25 | loss   nan | ppl      nan
| batch     2 | wps 558.85 | loss   nan | ppl      nan
| batch     3 | wps 561.00 | loss   nan | ppl      nan
| batch     4 | wps 560.37 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 13.44s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 554.48.
Elapsed_time(s) is 13.44.
Peak allocated bytes on cuda:0: 9.608905GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 109.17 | loss   nan | ppl      nan
| batch     2 | wps 186.55 | loss   nan | ppl      nan
| batch     3 | wps 180.91 | loss   nan | ppl      nan
| batch     4 | wps 186.96 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 35.92s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 181.66.
Elapsed_time(s) is 35.92.
Peak allocated bytes on cuda:0: 5.985140GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 111.19 | loss   nan | ppl      nan
| batch     2 | wps 180.99 | loss   nan | ppl      nan
| batch     3 | wps 176.30 | loss   nan | ppl      nan
| batch     4 | wps 181.46 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 36.01s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 177.36.
Elapsed_time(s) is 36.01.
Peak allocated bytes on cuda:0: 5.984896GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='EleutherAI_gpt-neo-1.3B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 1422.96 | loss   nan | ppl      nan
| batch     2 | wps 1944.17 | loss   nan | ppl      nan
| batch     3 | wps 1900.98 | loss   nan | ppl      nan
| batch     4 | wps 1969.68 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time:  6.44s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 1912.80.
Elapsed_time(s) is 6.44.
Peak allocated bytes on cuda:0: 16.034986GB
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='EleutherAI_gpt-neo-1.3B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 538.72 | loss   nan | ppl      nan
| batch     2 | wps 1117.60 | loss   nan | ppl      nan
| batch     3 | wps 1100.30 | loss   nan | ppl      nan
| batch     4 | wps 1116.84 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 13.18s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 1107.70.
Elapsed_time(s) is 13.18.
Peak allocated bytes on cuda:0: 8.377929GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='EleutherAI_gpt-neo-2.7B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='EleutherAI_gpt-neo-2.7B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 353.44 | loss   nan | ppl      nan
| batch     2 | wps 609.47 | loss   nan | ppl      nan
| batch     3 | wps 617.91 | loss   nan | ppl      nan
| batch     4 | wps 615.75 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 21.65s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 612.77.
Elapsed_time(s) is 21.65.
Peak allocated bytes on cuda:0: 16.231836GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='EleutherAI_gpt-neo-2.7B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 202.03 | loss   nan | ppl      nan
| batch     2 | wps 425.86 | loss   nan | ppl      nan
| batch     3 | wps 443.61 | loss   nan | ppl      nan
| batch     4 | wps 445.61 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 34.70s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 438.28.
Elapsed_time(s) is 34.70.
Peak allocated bytes on cuda:0: 5.250390GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='EleutherAI_gpt-neo-2.7B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 165.96 | loss   nan | ppl      nan
| batch     2 | wps 308.21 | loss   nan | ppl      nan
| batch     3 | wps 317.70 | loss   nan | ppl      nan
| batch     4 | wps 317.56 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 44.62s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 314.10.
Elapsed_time(s) is 44.62.
Peak allocated bytes on cuda:0: 5.248690GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 159.62 | loss   nan | ppl      nan
| batch     2 | wps 369.85 | loss   nan | ppl      nan
| batch     3 | wps 374.98 | loss   nan | ppl      nan
| batch     4 | wps 374.95 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 84.81s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 372.12.
Elapsed_time(s) is 84.81.
Peak allocated bytes on cuda:0: 14.755874GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 113.04 | loss   nan | ppl      nan
| batch     2 | wps 263.25 | loss   nan | ppl      nan
| batch     3 | wps 262.60 | loss   nan | ppl      nan
| batch     4 | wps 262.58 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 119.63s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 262.78.
Elapsed_time(s) is 119.63.
Peak allocated bytes on cuda:0: 14.777847GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-13b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 71.81 | loss   nan | ppl      nan
| batch     2 | wps 114.51 | loss   nan | ppl      nan
| batch     3 | wps 114.30 | loss   nan | ppl      nan
| batch     4 | wps 114.42 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 111.15s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 114.29.
Elapsed_time(s) is 111.15.
Peak allocated bytes on cuda:0: 13.074843GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=True, initrange=0.1, lr=0.0001, max_batch=4, model_config='Llama-2-13b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 44.80 | loss   nan | ppl      nan
| batch     2 | wps 84.89 | loss   nan | ppl      nan
| batch     3 | wps 84.38 | loss   nan | ppl      nan
| batch     4 | wps 84.16 | loss   nan | ppl      nan
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 164.56s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 84.43.
Elapsed_time(s) is 164.56.
Peak allocated bytes on cuda:0: 13.070516GB
