WORLD_SIZE=32
MASTER_ADDR=udc-aj33-9
Peak allocated bytes on cuda:1: 20.265346GB
Peak allocated bytes on cuda:2: 20.265346GB
Peak allocated bytes on cuda:3: 20.265346GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-large', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 100.47 | loss 25.90 | ppl 176551294027.54
| batch     2 | wps 84.76 | loss 12.54 | ppl 278034.51
| batch     3 | wps 86.12 | loss 12.08 | ppl 176215.84
| batch     4 | wps 89.60 | loss 12.04 | ppl 169774.25
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 56.17s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 85.72.
Elapsed_time(s) is 56.17.
Peak allocated bytes on cuda:0: 20.265346GB
Peak allocated bytes on cuda:1: 11.033508GB
Peak allocated bytes on cuda:2: 11.033508GB
Peak allocated bytes on cuda:3: 11.033508GB
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-large', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 144.64 | loss 25.90 | ppl 176551294027.54
| batch     2 | wps 263.36 | loss 12.54 | ppl 278034.51
| batch     3 | wps 278.80 | loss 12.08 | ppl 176214.67
| batch     4 | wps 225.69 | loss 12.04 | ppl 169751.91
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 26.31s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 247.63.
Elapsed_time(s) is 26.31.
Peak allocated bytes on cuda:0: 11.033508GB
Peak allocated bytes on cuda:1: 3.493508GB
Peak allocated bytes on cuda:3: 3.493508GB
Peak allocated bytes on cuda:2: 3.493508GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-large', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 60.89 | loss 25.90 | ppl 176405038994.12
| batch     2 | wps 95.89 | loss 12.53 | ppl 276948.18
| batch     3 | wps 99.02 | loss 12.08 | ppl 176367.16
| batch     4 | wps 93.54 | loss 12.05 | ppl 170316.21
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 66.00s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 95.46.
Elapsed_time(s) is 66.00.
Peak allocated bytes on cuda:0: 3.493508GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 100.15 | loss 26.80 | ppl 434496152512.53
| batch     2 | wps 150.48 | loss 13.04 | ppl 459464.96
| batch     3 | wps 122.71 | loss 12.56 | ppl 284071.18
| batch     4 | wps 116.03 | loss 12.31 | ppl 222391.12
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 44.48s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 129.90.
Elapsed_time(s) is 44.48.
Peak allocated bytes on cuda:0: 19.420421GB
Peak allocated bytes on cuda:1: 19.420421GB
Peak allocated bytes on cuda:2: 19.420421GB
Peak allocated bytes on cuda:3: 19.420421GB
Peak allocated bytes on cuda:3: 5.394612GB
Peak allocated bytes on cuda:1: 5.393934GB
Peak allocated bytes on cuda:2: 5.393934GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 43.41 | loss 26.80 | ppl 434061284066.09
| batch     2 | wps 85.62 | loss 13.03 | ppl 457891.96
| batch     3 | wps 82.41 | loss 12.56 | ppl 285666.17
| batch     4 | wps 85.95 | loss 12.33 | ppl 225882.72
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 83.96s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 84.27.
Elapsed_time(s) is 83.96.
Peak allocated bytes on cuda:0: 5.394127GB
Peak allocated bytes on cuda:2: 5.393907GB
Peak allocated bytes on cuda:1: 5.393907GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='gpt2-xl', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 41.86 | loss 26.80 | ppl 434061284066.09
| batch     2 | wps 70.81 | loss 13.04 | ppl 459785.82
| batch     3 | wps 71.69 | loss 12.57 | ppl 287213.95
| batch     4 | wps 73.50 | loss 12.33 | ppl 225771.80
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 92.09s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 71.34.
Elapsed_time(s) is 92.09.
Peak allocated bytes on cuda:0: 5.393907GB
Peak allocated bytes on cuda:3: 5.394127GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='EleutherAI_gpt-neo-1.3B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Peak allocated bytes on cuda:2: 18.246481GB
Peak allocated bytes on cuda:1: 18.246481GB
Peak allocated bytes on cuda:3: 18.246481GB
Running FSDP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='EleutherAI_gpt-neo-1.3B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 181.01 | loss 28.22 | ppl 1811044816228.61
| batch     2 | wps 265.10 | loss 13.36 | ppl 636582.12
| batch     3 | wps 286.77 | loss 12.72 | ppl 335564.96
| batch     4 | wps 271.05 | loss 12.30 | ppl 220247.97
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 45.11s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 275.94.
Elapsed_time(s) is 45.11.
Peak allocated bytes on cuda:0: 18.246481GB
Running TP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='EleutherAI_gpt-neo-2.7B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Peak allocated bytes on cuda:2: 10.994451GB
Peak allocated bytes on cuda:1: 10.994451GB
Peak allocated bytes on cuda:3: 10.994451GB
Peak allocated bytes on cuda:2: 11.004749GB
Peak allocated bytes on cuda:3: 11.004749GB
Peak allocated bytes on cuda:1: 11.004749GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='EleutherAI_gpt-neo-2.7B', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 64.08 | loss 30.15 | ppl 12461673051162.10
| batch     2 | wps 113.04 | loss 13.99 | ppl 1187841.09
| batch     3 | wps 114.82 | loss 13.31 | ppl 605776.97
| batch     4 | wps 115.90 | loss 12.86 | ppl 383106.25
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 117.92s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 115.29.
Elapsed_time(s) is 117.92.
Peak allocated bytes on cuda:0: 11.004749GB
Running DP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
Running TP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-7b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running TP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-13b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Peak allocated bytes on cuda:2: 26.599856GB
Peak allocated bytes on cuda:1: 26.599856GB
Peak allocated bytes on cuda:3: 26.599856GB
Running RTP-out-of-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-13b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 26.78 | loss 38.29 | ppl 42684492496837192.00
| batch     2 | wps 46.20 | loss 17.21 | ppl 29915126.60
| batch     3 | wps 45.57 | loss 17.13 | ppl 27505856.82
| batch     4 | wps 46.16 | loss 15.78 | ppl 7164056.34
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 287.02s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 45.87.
Elapsed_time(s) is 287.02.
Peak allocated bytes on cuda:0: 26.599856GB
Peak allocated bytes on cuda:1: 26.603804GB
Peak allocated bytes on cuda:2: 26.603804GB
Running RTP-in-place benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-13b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
| batch     1 | wps 22.63 | loss 38.29 | ppl 42684492496837192.00
| batch     2 | wps 34.00 | loss 17.21 | ppl 29902405.25
| batch     3 | wps 33.94 | loss 17.15 | ppl 28192049.19
| batch     4 | wps 34.24 | loss 15.81 | ppl 7363823.99
--------------------------------------------------------------------------------------------------------------
| end of epoch 1 | time: 361.86s 
--------------------------------------------------------------------------------------------------------------
Throughput(wps) is 34.04.
Elapsed_time(s) is 361.86.
Peak allocated bytes on cuda:0: 26.603781GB
Peak allocated bytes on cuda:3: 26.603781GB
Running TP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-30b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
Running TP benchmark with args: Namespace(batch_size=1, benchmark_eval=False, clip_value=0.05, debug=False, dropout=0, dry_run=False, enable_auto_wrap=False, epochs=1, full_fp16=False, initrange=0.1, lr=0.001, max_batch=4, model_config='Llama-2-70b', model_name='lm', nhead=32, nhid=5120, ninp=1280, num_decoder_layers=36, seq_len=1025, use_synthetic_data=True, vocab_size=50256)
--------------------------------------------------------------------------------------------------------------
| start of epoch 1
--------------------------------------------------------------------------------------------------------------
