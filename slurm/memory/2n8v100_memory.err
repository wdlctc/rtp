Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
srun: error: udc-aj40-36: tasks 4,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.0
slurmstepd: error: *** STEP 55131466.0 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:18:54 ***
srun: error: udc-aj40-36: task 6: Exited with exit code 1
srun: error: udc-aj38-36: tasks 1-3: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: Force Terminated StepId=55131466.0
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
srun: error: udc-aj40-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.1
slurmstepd: error: *** STEP 55131466.1 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:19:27 ***
srun: error: udc-aj40-36: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacty of 31.74 GiB of which 625.62 MiB is free. Including non-PyTorch memory, this process has 31.12 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 7.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 3 has a total capacty of 31.74 GiB of which 755.62 MiB is free. Including non-PyTorch memory, this process has 31.00 GiB memory in use. Of the allocated memory 22.64 GiB is allocated by PyTorch, and 7.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacty of 31.74 GiB of which 789.62 MiB is free. Including non-PyTorch memory, this process has 30.96 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 7.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 2 has a total capacty of 31.74 GiB of which 641.62 MiB is free. Including non-PyTorch memory, this process has 31.11 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 7.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 3 has a total capacty of 31.74 GiB of which 445.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 8.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 2 has a total capacty of 31.74 GiB of which 921.62 MiB is free. Including non-PyTorch memory, this process has 30.84 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 7.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacty of 31.74 GiB of which 991.62 MiB is free. Including non-PyTorch memory, this process has 30.77 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 7.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 8.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.2
slurmstepd: error: *** STEP 55131466.2 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:20:08 ***
srun: error: udc-aj38-36: tasks 0,2: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: task 1: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-36: tasks 4-6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.3
slurmstepd: error: *** STEP 55131466.3 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:20:45 ***
srun: error: udc-aj38-36: tasks 0-2: Terminated
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Terminated
srun: Force Terminated StepId=55131466.3
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-36: tasks 5,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.4
slurmstepd: error: *** STEP 55131466.4 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:21:24 ***
srun: error: udc-aj40-36: task 6: Exited with exit code 1
srun: error: udc-aj38-36: tasks 1-3: Terminated
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Terminated
srun: Force Terminated StepId=55131466.4
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: tasks 4,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.5
slurmstepd: error: *** STEP 55131466.5 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:21:53 ***
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: task 6: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
srun: error: udc-aj40-36: tasks 4-5,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.6
slurmstepd: error: *** STEP 55131466.6 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:22:49 ***
srun: error: udc-aj40-36: task 6: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacty of 31.74 GiB of which 401.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.36 GiB is allocated by PyTorch, and 499.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 2 has a total capacty of 31.74 GiB of which 201.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.36 GiB is allocated by PyTorch, and 699.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 3 has a total capacty of 31.74 GiB of which 405.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 502.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacty of 31.74 GiB of which 247.62 MiB is free. Including non-PyTorch memory, this process has 31.49 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 674.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacty of 31.74 GiB of which 201.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 702.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 2 has a total capacty of 31.74 GiB of which 279.62 MiB is free. Including non-PyTorch memory, this process has 31.46 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 627.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 3 has a total capacty of 31.74 GiB of which 205.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 701.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 180, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacty of 31.74 GiB of which 221.62 MiB is free. Including non-PyTorch memory, this process has 31.52 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 702.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: tasks 5-6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.7
slurmstepd: error: *** STEP 55131466.7 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:23:12 ***
srun: error: udc-aj38-36: tasks 1,3: Exited with exit code 1
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: task 2: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.8
slurmstepd: error: *** STEP 55131466.8 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:24:03 ***
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: tasks 0,2: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: task 1: Terminated
srun: Force Terminated StepId=55131466.8
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.9
slurmstepd: error: *** STEP 55131466.9 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:25:01 ***
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: tasks 1-3: Terminated
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Terminated
srun: Force Terminated StepId=55131466.9
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.10
slurmstepd: error: *** STEP 55131466.10 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:25:31 ***
srun: error: udc-aj40-36: tasks 4,6-7: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
srun: error: udc-aj40-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.11
slurmstepd: error: *** STEP 55131466.11 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:26:22 ***
srun: error: udc-aj40-36: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 179.62 MiB is free. Including non-PyTorch memory, this process has 31.56 GiB memory in use. Of the allocated memory 30.15 GiB is allocated by PyTorch, and 948.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 30.15 GiB is allocated by PyTorch, and 756.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 287.62 MiB is free. Including non-PyTorch memory, this process has 31.46 GiB memory in use. Of the allocated memory 30.15 GiB is allocated by PyTorch, and 820.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 163.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.15 GiB is allocated by PyTorch, and 948.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 163.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.15 GiB is allocated by PyTorch, and 948.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.15 GiB is allocated by PyTorch, and 948.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 2-3: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.12
slurmstepd: error: *** STEP 55131466.12 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:26:45 ***
srun: error: udc-aj40-36: tasks 4-6: Exited with exit code 1
srun: error: udc-aj38-36: task 1: Terminated
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Terminated
srun: Force Terminated StepId=55131466.12
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.13
slurmstepd: error: *** STEP 55131466.13 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:27:37 ***
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: tasks 1-3: Terminated
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Terminated
srun: Force Terminated StepId=55131466.13
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-36: tasks 4-6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.14
slurmstepd: error: *** STEP 55131466.14 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:28:33 ***
srun: error: udc-aj38-36: tasks 0-2: Terminated
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Terminated
srun: Force Terminated StepId=55131466.14
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1,3: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.15
slurmstepd: error: *** STEP 55131466.15 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:29:09 ***
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: error: udc-aj38-36: task 2: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 9.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 9.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.10 GiB is allocated by PyTorch, and 137.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: task 6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.16
slurmstepd: error: *** STEP 55131466.16 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:29:47 ***
srun: error: udc-aj38-36: tasks 0,2: Exited with exit code 1
srun: error: udc-aj40-36: tasks 4,7: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: task 1: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 167, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 578, in forward
    attn_output, attn_output_weights = self.MultiheadAttention(query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 495, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 29.54 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 167, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 578, in forward
    attn_output, attn_output_weights = self.MultiheadAttention(query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 495, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 93.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 29.54 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 167, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 578, in forward
    attn_output, attn_output_weights = self.MultiheadAttention(query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 495, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 353.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 29.54 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 167, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 578, in forward
    attn_output, attn_output_weights = self.MultiheadAttention(query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 495, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 337.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 29.54 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 167, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 578, in forward
    attn_output, attn_output_weights = self.MultiheadAttention(query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 495, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 413.62 MiB is free. Including non-PyTorch memory, this process has 31.33 GiB memory in use. Of the allocated memory 29.54 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1,3: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.17
slurmstepd: error: *** STEP 55131466.17 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:30:09 ***
srun: error: udc-aj40-36: tasks 5,7: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 6: Terminated
srun: error: udc-aj38-36: task 2: Terminated
srun: Force Terminated StepId=55131466.17
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 111.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.10 GiB is allocated by PyTorch, and 997.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 153.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 30.11 GiB is allocated by PyTorch, and 974.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 145.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 966.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 199.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.10 GiB is allocated by PyTorch, and 947.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 43.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 30.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 91.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 30.10 GiB is allocated by PyTorch, and 1019.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 173.62 MiB is free. Including non-PyTorch memory, this process has 31.57 GiB memory in use. Of the allocated memory 30.11 GiB is allocated by PyTorch, and 953.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 229.62 MiB is free. Including non-PyTorch memory, this process has 31.51 GiB memory in use. Of the allocated memory 30.10 GiB is allocated by PyTorch, and 918.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.18
slurmstepd: error: *** STEP 55131466.18 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:31:02 ***
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 47.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 30.05 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 81.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 53.62 MiB is free. Including non-PyTorch memory, this process has 31.68 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 81.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 99.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 1013.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.19
slurmstepd: error: *** STEP 55131466.19 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:31:59 ***
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: Force Terminated StepId=55131466.19
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 0-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.20
slurmstepd: error: *** STEP 55131466.20 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:33:10 ***
srun: error: udc-aj40-36: tasks 5-7: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.21
slurmstepd: error: *** STEP 55131466.21 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:34:05 ***
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: tasks 5-6: Terminated
srun: error: udc-aj38-36: tasks 1-2: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: Force Terminated StepId=55131466.21
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 29.84 GiB memory in use. Of the allocated memory 28.21 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 29.84 GiB memory in use. Of the allocated memory 28.21 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.92 GiB is free. Including non-PyTorch memory, this process has 29.82 GiB memory in use. Of the allocated memory 28.21 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.22
slurmstepd: error: *** STEP 55131466.22 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:34:30 ***
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: tasks 6-7: Terminated
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 5: Terminated
srun: Force Terminated StepId=55131466.22
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 33.62 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 388.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 73.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 356.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 33.62 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 388.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 105.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 356.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 33.62 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 388.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 33.62 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 388.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 105.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 356.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 73.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.80 GiB is allocated by PyTorch, and 356.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.23
slurmstepd: error: *** STEP 55131466.23 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:35:48 ***
srun: error: udc-aj40-36: tasks 6-7: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: task 5: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 67.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 386.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 139.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 354.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 67.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 386.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 107.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 354.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 67.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 386.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 67.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 386.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 107.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 354.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 139.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.77 GiB is allocated by PyTorch, and 354.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: tasks 4-5: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.24
srun: error: udc-aj38-36: tasks 2-3: Exited with exit code 1
slurmstepd: error: *** STEP 55131466.24 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:37:05 ***
srun: error: udc-aj38-36: task 1: Exited with exit code 1
srun: error: udc-aj40-36: task 6: Exited with exit code 1
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.25
slurmstepd: error: *** STEP 55131466.25 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:38:50 ***
srun: error: udc-aj40-36: task 5: Terminated
srun: error: udc-aj38-36: tasks 1-2: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: error: udc-aj40-36: task 6: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 4: Terminated
srun: Force Terminated StepId=55131466.25
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: task 1: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.26
slurmstepd: error: *** STEP 55131466.26 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:40:38 ***
srun: error: udc-aj40-36: task 6: Exited with exit code 1
srun: error: udc-aj40-36: task 7: Terminated
srun: error: udc-aj38-36: tasks 0,2: Terminated
srun: error: udc-aj40-36: task 5: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 4: Terminated
srun: Force Terminated StepId=55131466.26
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.31 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.31 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.31 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.31 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.31 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.24 GiB is free. Including non-PyTorch memory, this process has 30.50 GiB memory in use. Of the allocated memory 28.31 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.27
slurmstepd: error: *** STEP 55131466.27 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:41:10 ***
srun: error: udc-aj38-36: tasks 0-1,3: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj38-36: task 2: Terminated
srun: Force Terminated StepId=55131466.27
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 854.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 854.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 854.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 39.62 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 854.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 854.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 39.62 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 854.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.36 GiB is allocated by PyTorch, and 879.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.36 GiB is allocated by PyTorch, and 879.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.28
slurmstepd: error: *** STEP 55131466.28 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:43:28 ***
srun: error: udc-aj40-36: tasks 5-6: Exited with exit code 1
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: Force Terminated StepId=55131466.28
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.45 GiB is allocated by PyTorch, and 773.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.45 GiB is allocated by PyTorch, and 773.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 47.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 30.45 GiB is allocated by PyTorch, and 773.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 15.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.45 GiB is allocated by PyTorch, and 773.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-36: tasks 5-6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.29
slurmstepd: error: *** STEP 55131466.29 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:45:48 ***
srun: error: udc-aj40-36: task 7: Exited with exit code 1
srun: error: udc-aj38-36: tasks 2-3: Terminated
srun: error: udc-aj38-36: task 1: Terminated
srun: error: udc-aj40-36: task 4: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Terminated
srun: Force Terminated StepId=55131466.29
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.30
slurmstepd: error: *** STEP 55131466.30 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:48:20 ***
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: tasks 5,7: Terminated
srun: error: udc-aj40-36: task 6: Killed
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj38-36: task 2: Terminated
srun: Force Terminated StepId=55131466.30
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.31
slurmstepd: error: *** STEP 55131466.31 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:50:55 ***
srun: error: udc-aj38-36: tasks 2-3: Terminated
srun: error: udc-aj40-36: task 5: Killed
srun: error: udc-aj40-36: task 7: Killed
srun: error: udc-aj40-36: task 6: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 4: Terminated
srun: Force Terminated StepId=55131466.31
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 705.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 205.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 497.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 205.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 497.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 593.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 705.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 209.62 MiB is free. Including non-PyTorch memory, this process has 31.53 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 705.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 205.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 497.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 205.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.55 GiB is allocated by PyTorch, and 497.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1,3: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.32
slurmstepd: error: *** STEP 55131466.32 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:51:38 ***
srun: error: udc-aj40-36: tasks 5-7: Exited with exit code 1
srun: error: udc-aj38-36: task 2: Exited with exit code 1
srun: error: udc-aj38-36: task 0: Exited with exit code 1
srun: error: udc-aj40-36: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.33
slurmstepd: error: *** STEP 55131466.33 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:54:09 ***
srun: error: udc-aj38-36: tasks 0,2: Terminated
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj40-36: task 7: Killed
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 6: Terminated
srun: error: udc-aj40-36: task 5: Terminated
srun: Force Terminated StepId=55131466.33
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 999, in __init__
    self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 531628032 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 65, in __init__
    self.linear1 = nn.Linear(d_model, dim_feedforward)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: tasks 1,3: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.34
slurmstepd: error: *** STEP 55131466.34 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:56:43 ***
srun: error: udc-aj38-36: task 2: Killed
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 7: Killed
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj40-36: task 5: Terminated
srun: error: udc-aj40-36: task 6: Terminated
srun: Force Terminated StepId=55131466.34
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1008, in __init__
    self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 128, in __init__
    super().__init__(in_features, out_features, bias=bias,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 268435456 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.35
slurmstepd: error: *** STEP 55131466.35 ON udc-aj38-36 CANCELLED AT 2023-11-19T17:59:17 ***
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj38-36: task 2: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: error: udc-aj40-36: tasks 4,6: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 5: Terminated
srun: Force Terminated StepId=55131466.35
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1008, in __init__
    self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 128, in __init__
    super().__init__(in_features, out_features, bias=bias,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 268435456 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.36
slurmstepd: error: *** STEP 55131466.36 ON udc-aj38-36 CANCELLED AT 2023-11-19T18:01:52 ***
srun: error: udc-aj38-36: task 2: Killed
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 4: Killed
srun: error: udc-aj40-36: task 6: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: error: udc-aj40-36: task 5: Terminated
srun: Force Terminated StepId=55131466.36
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 28, in forward
    return self.embedding(src) * self.ninp_sqrt
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 326, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 91.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.29 GiB is allocated by PyTorch, and 2.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 28, in forward
    return self.embedding(src) * self.ninp_sqrt
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 326, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 91.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.29 GiB is allocated by PyTorch, and 2.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 28, in forward
    return self.embedding(src) * self.ninp_sqrt
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 326, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 91.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.29 GiB is allocated by PyTorch, and 2.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-36: tasks 1-2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131466.37
slurmstepd: error: *** STEP 55131466.37 ON udc-aj38-36 CANCELLED AT 2023-11-19T18:02:51 ***
srun: error: udc-aj38-36: task 3: Exited with exit code 1
srun: error: udc-aj40-36: tasks 5-6: Terminated
srun: error: udc-aj40-36: task 4: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: Force Terminated StepId=55131466.37
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1008, in __init__
    self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 128, in __init__
    super().__init__(in_features, out_features, bias=bias,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 268435456 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.38
slurmstepd: error: *** STEP 55131466.38 ON udc-aj38-36 CANCELLED AT 2023-11-19T18:05:23 ***
srun: error: udc-aj40-36: task 5: Terminated
srun: error: udc-aj38-36: tasks 0,2: Terminated
srun: error: udc-aj40-36: tasks 4,6: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: Force Terminated StepId=55131466.38
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1008, in __init__
    self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 128, in __init__
    super().__init__(in_features, out_features, bias=bias,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 268435456 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj38-36: task 1: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131466.39
slurmstepd: error: *** STEP 55131466.39 ON udc-aj38-36 CANCELLED AT 2023-11-19T18:07:58 ***
srun: error: udc-aj38-36: task 2: Terminated
srun: error: udc-aj40-36: task 7: Terminated
srun: error: udc-aj38-36: task 3: Terminated
srun: error: udc-aj40-36: tasks 4,6: Terminated
srun: error: udc-aj38-36: task 0: Terminated
srun: error: udc-aj40-36: task 5: Killed
srun: Force Terminated StepId=55131466.39
