Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
srun: error: udc-aj37-36: tasks 21,23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.0
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
slurmstepd: error: *** STEP 55131464.0 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:01:10 ***
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: error: udc-aj36-35: tasks 12,14: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-6: Exited with exit code 1
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9,11: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Terminated
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Terminated
srun: Force Terminated StepId=55131464.0
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
srun: error: udc-aj40-35: tasks 29-31: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.1
slurmstepd: error: *** STEP 55131464.1 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:01:55 ***
srun: error: udc-aj37-36: tasks 21-23: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-15: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-19: Exited with exit code 1
srun: error: udc-aj34-35: tasks 8-10: Exited with exit code 1
srun: error: udc-aj38-35: tasks 24,26-27: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-7: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 621.62 MiB is free. Including non-PyTorch memory, this process has 31.13 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 557.62 MiB is free. Including non-PyTorch memory, this process has 31.19 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 577.62 MiB is free. Including non-PyTorch memory, this process has 31.17 GiB memory in use. Of the allocated memory 29.48 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 637.62 MiB is free. Including non-PyTorch memory, this process has 31.11 GiB memory in use. Of the allocated memory 29.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 597.62 MiB is free. Including non-PyTorch memory, this process has 31.15 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 477.62 MiB is free. Including non-PyTorch memory, this process has 31.27 GiB memory in use. Of the allocated memory 29.48 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 457.62 MiB is free. Including non-PyTorch memory, this process has 31.29 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 501.62 MiB is free. Including non-PyTorch memory, this process has 31.25 GiB memory in use. Of the allocated memory 29.48 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 481.62 MiB is free. Including non-PyTorch memory, this process has 31.27 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 537.62 MiB is free. Including non-PyTorch memory, this process has 31.21 GiB memory in use. Of the allocated memory 29.48 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 597.62 MiB is free. Including non-PyTorch memory, this process has 31.15 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 601.62 MiB is free. Including non-PyTorch memory, this process has 31.15 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 637.62 MiB is free. Including non-PyTorch memory, this process has 31.11 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 477.62 MiB is free. Including non-PyTorch memory, this process has 31.27 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 581.62 MiB is free. Including non-PyTorch memory, this process has 31.17 GiB memory in use. Of the allocated memory 29.47 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 637.62 MiB is free. Including non-PyTorch memory, this process has 31.11 GiB memory in use. Of the allocated memory 29.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj36-35: task 14: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.2
slurmstepd: error: *** STEP 55131464.2 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:02:33 ***
srun: error: udc-aj37-36: tasks 21-23: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj40-35: tasks 28-30: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25,27: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Terminated
srun: error: udc-aj36-35: tasks 12-13: Terminated
srun: error: udc-aj33-10: tasks 6-7: Terminated
srun: error: udc-aj33-9: tasks 1-2: Terminated
srun: error: udc-aj33-10: task 5: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj38-35: task 26: Terminated
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj34-35: task 8: Terminated
srun: error: udc-aj33-10: task 4: Terminated
srun: Force Terminated StepId=55131464.2
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
srun: error: udc-aj37-36: tasks 20-22: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.3
slurmstepd: error: *** STEP 55131464.3 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:03:50 ***
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj40-35: tasks 28,30: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25,27: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-10: tasks 6-7: Exited with exit code 1
srun: error: udc-aj33-9: tasks 2-3: Terminated
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Terminated
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj38-35: task 26: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Terminated
srun: Force Terminated StepId=55131464.3
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 20-21,23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.4
srun: error: udc-aj40-35: tasks 29-31: Exited with exit code 1
slurmstepd: error: *** STEP 55131464.4 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:05:13 ***
srun: error: udc-aj37-35: tasks 18-19: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: error: udc-aj34-35: tasks 8,10: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-15: Exited with exit code 1
srun: error: udc-aj33-10: tasks 4-5: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Terminated
srun: Force Terminated StepId=55131464.4
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 20,22: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.5
slurmstepd: error: *** STEP 55131464.5 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:05:55 ***
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj38-35: tasks 26-27: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj36-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj34-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5,7: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
srun: error: udc-aj37-36: tasks 21-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.6
slurmstepd: error: *** STEP 55131464.6 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:07:07 ***
srun: error: udc-aj40-35: tasks 29-31: Exited with exit code 1
srun: error: udc-aj37-35: tasks 16,18: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-27: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj34-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj33-10: tasks 6-7: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 321.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 29.34 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 341.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 29.34 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 325.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 29.34 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 321.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 29.34 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 321.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 29.34 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 325.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 29.34 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: tasks 29,31: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.7
slurmstepd: error: *** STEP 55131464.7 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:07:42 ***
srun: error: udc-aj38-35: tasks 26-27: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-36: tasks 20,22-23: Terminated
srun: error: udc-aj38-35: task 25: Terminated
srun: error: udc-aj37-35: tasks 18-19: Terminated
srun: error: udc-aj36-35: tasks 13-14: Terminated
srun: error: udc-aj34-35: tasks 10-11: Terminated
srun: error: udc-aj33-10: tasks 5-6: Terminated
srun: error: udc-aj33-9: tasks 2-3: Terminated
srun: error: udc-aj37-35: task 16: Terminated
srun: error: udc-aj34-35: task 8: Terminated
srun: error: udc-aj36-35: task 12: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj40-35: task 30: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj37-36: task 21: Terminated
srun: error: udc-aj33-10: task 4: Terminated
srun: error: udc-aj37-35: task 17: Terminated
srun: error: udc-aj33-9: task 1: Terminated
srun: error: udc-aj34-35: task 9: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: Force Terminated StepId=55131464.7
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 20-21: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.8
slurmstepd: error: *** STEP 55131464.8 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:09:32 ***
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-31: Exited with exit code 1
srun: error: udc-aj38-35: tasks 26-27: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17,19: Exited with exit code 1
srun: error: udc-aj33-10: tasks 6-7: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13,15: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Exited with exit code 1
srun: error: udc-aj34-35: tasks 8,11: Exited with exit code 1
srun: error: udc-aj36-35: task 14: Exited with exit code 1
srun: error: udc-aj33-9: tasks 2-3: Terminated
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Terminated
srun: Force Terminated StepId=55131464.8
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.9
slurmstepd: error: *** STEP 55131464.9 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:11:28 ***
srun: error: udc-aj37-36: tasks 22-23: Exited with exit code 1
srun: error: udc-aj38-35: tasks 26-27: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj33-10: tasks 4-5: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17,19: Terminated
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj36-35: tasks 12-13: Terminated
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Terminated
srun: error: udc-aj33-9: tasks 0-1: Terminated
srun: error: udc-aj37-35: task 16: Terminated
srun: error: udc-aj36-35: task 14: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj37-35: task 18: Terminated
srun: error: udc-aj33-9: task 2: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: Force Terminated StepId=55131464.9
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
*** Error in `/scratch/fad3ew/rtp/.venv/bin/python': free(): invalid pointer: 0x00005652abe6c590 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x81329)[0x7f1632bf9329]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/../../nvidia/cuda_runtime/lib/libcudart.so.12(+0x23e27)[0x7f15f189ce27]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/../../nvidia/cuda_runtime/lib/libcudart.so.12(+0x27361)[0x7f15f18a0361]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/../../nvidia/cuda_runtime/lib/libcudart.so.12(cudaEventQuery+0x41)[0x7f15f18c6f61]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so(_ZNK4c10d16ProcessGroupNCCL8WorkNCCL28finishedGPUExecutionInternalEv+0x55)[0x7f15ac974215]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so(_ZN4c10d16ProcessGroupNCCL8WorkNCCL11isCompletedEv+0x58)[0x7f15ac978068]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so(_ZN4c10d16ProcessGroupNCCL15workCleanupLoopEv+0x250)[0x7f15ac98e900]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so(_ZN4c10d16ProcessGroupNCCL16ncclCommWatchdogEv+0x78)[0x7f15ac98ec08]
/sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/bin/../lib/libstdc++.so.6(+0xc9067)[0x7f15f1164067]
/lib64/libpthread.so.0(+0x7ea5)[0x7f163385eea5]
/lib64/libc.so.6(clone+0x6d)[0x7f1632c76b0d]
======= Memory map: ========
200000000-200400000 ---p 00000000 00:00 0 
200400000-200600000 rw-s 00000000 00:05 39723                            /dev/nvidia3
200600000-204e00000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
204e00000-205e00000 ---p 00000000 00:00 0 
205e00000-206000000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
206000000-206200000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
206200000-206400000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
206400000-206600000 rw-s 206400000 00:05 36029                           /dev/nvidia-uvm
206600000-206800000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
206800000-206a00000 ---p 00000000 00:00 0 
206a00000-206c00000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
206c00000-206e00000 ---p 00000000 00:00 0 
206e00000-207000000 rw-s 00000000 00:04 853115414                        /dev/zero (deleted)
207000000-207400000 ---p 00000000 00:00 0 
207400000-207600000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
207600000-207800000 ---p 00000000 00:00 0 
207800000-207a00000 rw-s 00000000 00:05 17812                            /dev/nvidiactl
207a00000-600200000 ---p 00000000 00:00 0 
10000000000-10004000000 ---p 00000000 00:00 0 
5652a5423000-5652a5482000 r--p 00000000 00:2b 250134081                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/bin/python3.8
5652a5482000-5652a5697000 r-xp 0005f000 00:2b 250134081                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/bin/python3.8
5652a5697000-5652a5789000 r--p 00274000 00:2b 250134081                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/bin/python3.8
5652a578a000-5652a578f000 r--p 00366000 00:2b 250134081                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/bin/python3.8
5652a578f000-5652a57c7000 rw-p 0036b000 00:2b 250134081                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/bin/python3.8
5652a57c7000-5652a57e7000 rw-p 00000000 00:00 0 
5652a5eda000-5652c6a11000 rw-p 00000000 00:00 0                          [heap]
7f0cda000000-7f0f30000000 ---p 00000000 00:00 0 
7f0f30000000-7f0f30021000 rw-p 00000000 00:00 0 
7f0f30021000-7f0f34000000 ---p 00000000 00:00 0 
7f0f34000000-7f0f34021000 rw-p 00000000 00:00 0 
7f0f34021000-7f0f38000000 ---p 00000000 00:00 0 
7f0f38000000-7f0f38021000 rw-p 00000000 00:00 0 
7f0f38021000-7f0f3c000000 ---p 00000000 00:00 0 
7f0f3e000000-7f110d200000 ---p 00000000 00:00 0 
7f110d200000-7f110d400000 rw-s 00000000 00:04 857781648                  /dev/zero (deleted)
7f110d400000-7f110d901000 rw-s 00000000 00:04 857781649                  /dev/zero (deleted)
7f110d901000-7f13be000000 ---p 00000000 00:00 0 
7f13be000000-7f13c0000000 rw-s 00000000 00:04 854847086                  /dev/zero (deleted)
7f13c0000000-7f13c0021000 rw-p 00000000 00:00 0 
7f13c0021000-7f13c4000000 ---p 00000000 00:00 0 
7f13c6000000-7f13dea00000 ---p 00000000 00:00 0 
7f13dea00000-7f13dec00000 rw-s 00000000 00:04 853115420                  /dev/zero (deleted)
7f13dec00000-7f1402000000 ---p 00000000 00:00 0 
7f1402ffe000-7f1402fff000 ---p 00000000 00:00 0 
7f1402fff000-7f14037ff000 rw-p 00000000 00:00 0 
7f14037ff000-7f1403800000 ---p 00000000 00:00 0 
7f1403800000-7f1404000000 rw-p 00000000 00:00 0 
7f1404000000-7f1408000000 ---p 00000000 00:00 0 
7f1408000000-7f1408021000 rw-p 00000000 00:00 0 
7f1408021000-7f140c000000 ---p 00000000 00:00 0 
7f140c3ff000-7f140c43f000 rw-p 00000000 00:00 0 
7f140c43f000-7f140c440000 ---p 00000000 00:00 0 
7f140c440000-7f140cc80000 rw-p 00000000 00:00 0 
7f140e000000-7f1410000000 ---p 00000000 00:00 0 
7f1410000000-7f1413a56000 rw-p 00000000 00:00 0 
7f1413a56000-7f1414000000 ---p 00000000 00:00 0 
7f1414820000-7f141fffb000 rw-p 00000000 00:00 0 
7f141fffb000-7f1420000000 ---p 00000000 00:00 0 
7f1422000000-7f1550000000 ---p 00000000 00:00 0 
7f1550000000-7f1550021000 rw-p 00000000 00:00 0 
7f1550021000-7f1554000000 ---p 00000000 00:00 0 
7f15543fd000-7f1554bbd000 rw-p 00000000 00:00 0 
7f1554bfd000-7f1554ffe000 rw-s 00000000 00:12 854847085                  /dev/shm/nccl-XVM4CK (deleted)
7f1554ffe000-7f1554fff000 ---p 00000000 00:00 0 
7f1554fff000-7f15557ff000 rw-p 00000000 00:00 0 
7f15557ff000-7f1555800000 ---p 00000000 00:00 0 
7f1555800000-7f1556000000 rw-p 00000000 00:00 0 
7f1556000000-7f1556400000 ---p 00000000 00:00 0 
7f1556400000-7f1556600000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1556600000-7f1556800000 rw-s 00000000 00:04 853115411                  /dev/zero (deleted)
7f1556800000-7f1556a00000 rw-s 00000000 00:04 853115412                  /dev/zero (deleted)
7f1556a00000-7f1557000000 ---p 00000000 00:00 0 
7f1557000000-7f1557200000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1557200000-7f1557400000 rw-s 00000000 00:04 853115415                  /dev/zero (deleted)
7f1557400000-7f1557619000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1557619000-7f1558000000 ---p 00000000 00:00 0 
7f1558000000-7f1558021000 rw-p 00000000 00:00 0 
7f1558021000-7f155c000000 ---p 00000000 00:00 0 
7f155c000000-7f155c021000 rw-p 00000000 00:00 0 
7f155c021000-7f1570000000 ---p 00000000 00:00 0 
7f15700b5000-7f15701f5000 rw-p 00000000 00:00 0 
7f15701f5000-7f15705f6000 rw-s 00000000 00:12 854847085                  /dev/shm/nccl-XVM4CK (deleted)
7f15705f6000-7f15705f7000 ---p 00000000 00:00 0 
7f15705f7000-7f1570df7000 rw-p 00000000 00:00 0 
7f1570df7000-7f1570e67000 r-xp 00000000 00:24 40385759                   /usr/lib64/libmlx5.so.1.24.43.0
7f1570e67000-7f1571067000 ---p 00070000 00:24 40385759                   /usr/lib64/libmlx5.so.1.24.43.0
7f1571067000-7f1571068000 r--p 00070000 00:24 40385759                   /usr/lib64/libmlx5.so.1.24.43.0
7f1571068000-7f1571069000 rw-p 00071000 00:24 40385759                   /usr/lib64/libmlx5.so.1.24.43.0
7f1571069000-7f157106c000 rw-p 00000000 00:00 0 
7f157106c000-7f157108a000 r-xp 00000000 00:24 40385803                   /usr/lib64/libnl-3.so.200.23.0
7f157108a000-7f157128a000 ---p 0001e000 00:24 40385803                   /usr/lib64/libnl-3.so.200.23.0
7f157128a000-7f157128c000 r--p 0001e000 00:24 40385803                   /usr/lib64/libnl-3.so.200.23.0
7f157128c000-7f157128d000 rw-p 00020000 00:24 40385803                   /usr/lib64/libnl-3.so.200.23.0
7f157128d000-7f15712f1000 r-xp 00000000 00:24 40385808                   /usr/lib64/libnl-route-3.so.200.23.0
7f15712f1000-7f15714f0000 ---p 00064000 00:24 40385808                   /usr/lib64/libnl-route-3.so.200.23.0
7f15714f0000-7f15714f3000 r--p 00063000 00:24 40385808                   /usr/lib64/libnl-route-3.so.200.23.0
7f15714f3000-7f15714f8000 rw-p 00066000 00:24 40385808                   /usr/lib64/libnl-route-3.so.200.23.0
7f15714f8000-7f15714fa000 rw-p 00000000 00:00 0 
7f15714fa000-7f1571518000 r-xp 00000000 00:24 40385638                   /usr/lib64/libibverbs.so.1.14.43.0
7f1571518000-7f1571717000 ---p 0001e000 00:24 40385638                   /usr/lib64/libibverbs.so.1.14.43.0
7f1571717000-7f1571718000 r--p 0001d000 00:24 40385638                   /usr/lib64/libibverbs.so.1.14.43.0
7f1571718000-7f1571719000 rw-p 0001e000 00:24 40385638                   /usr/lib64/libibverbs.so.1.14.43.0
7f1571719000-7f157171a000 ---p 00000000 00:00 0 
7f157171a000-7f1571f1a000 rw-p 00000000 00:00 0 
7f1571f1a000-7f1571f21000 r-xp 00000000 00:24 40385824                   /usr/lib64/libnss_db-2.17.so
7f1571f21000-7f1572120000 ---p 00007000 00:24 40385824                   /usr/lib64/libnss_db-2.17.so
7f1572120000-7f1572121000 r--p 00006000 00:24 40385824                   /usr/lib64/libnss_db-2.17.so
7f1572121000-7f1572122000 rw-p 00007000 00:24 40385824                   /usr/lib64/libnss_db-2.17.so
7f1572122000-7f1572125000 rw-p 00000000 00:00 0 
7f1572144000-7f1572204000 rw-p 00000000 00:00 0 
7f1573228000-7f1573375000 rw-p 00000000 00:00 0 
7f1573375000-7f1573575000 rw-s 00000000 00:04 853115413                  /dev/zero (deleted)
7f1573575000-7f1573576000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573576000-7f1573577000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573577000-7f1573578000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573578000-7f1573579000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573579000-7f157357a000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157357a000-7f157357b000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157357b000-7f157357c000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157357c000-7f157357d000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157357d000-7f157357e000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157357e000-7f157357f000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157357f000-7f1573580000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573580000-7f1573581000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573581000-7f1573582000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573582000-7f1573583000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573583000-7f1573584000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573584000-7f1573585000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573585000-7f1573586000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573586000-7f1573587000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573587000-7f1573588000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573588000-7f1573589000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573589000-7f157358a000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157358a000-7f157358b000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157358b000-7f157358c000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157358c000-7f157358d000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157358d000-7f157358e000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157358e000-7f157358f000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f157358f000-7f1573590000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573590000-7f1573591000 rw-s 00000000 00:05 17812                      /dev/nvidiactl
7f1573591000-7f1573816000 ---p 00000000 00:00 0 
7f1573816000-7f1573817000 ---p 00000000 00:00 0 
7f1573817000-7f1574057000 rw-p 00000000 00:00 0 
7f1574057000-7f1574058000 ---p 00000000 00:00 0 
7f1574058000-7f1574858000 rw-p 00000000 00:00 0 
7f1574858000-7f1574859000 ---p 00000000 00:00 0 
7f1574859000-7f1575059000 rw-p 00000000 00:00 0 
7f1575059000-7f157badd000 ---p 00000000 00:00 0 
7f157badd000-7f157baed000 -w-s 00000000 00:05 39723                      /dev/nvidia3
7f157baed000-7f157bafd000 -w-s 00000000 00:05 39722                      /dev/nvidia2
7f157bafd000-7f157bb0d000 -w-s 00000000 00:05 39721                      /dev/nvidia1
7f157bb0d000-7f157bb19000 r-xp 00000000 00:24 40385826                   /usr/lib64/libnss_files-2.17.so
7f157bb19000-7f157bd18000 ---p 0000c000 00:24 40385826                   /usr/lib64/libnss_files-2.17.so
7f157bd18000-7f157bd19000 r--p 0000b000 00:24 40385826                   /usr/lib64/libnss_files-2.17.so
7f157bd19000-7f157bd1a000 rw-p 0000c000 00:24 40385826                   /usr/lib64/libnss_files-2.17.so
7f157bd1a000-7f157bd20000 rw-p 00000000 00:00 0 
7f157bd20000-7f157beac000 r-xp 00000000 00:24 40382126                   /usr/lib64/libnvidia-ml.so.535.104.12
7f157beac000-7f157c0ab000 ---p 0018c000 00:24 40382126                   /usr/lib64/libnvidia-ml.so.535.104.12
7f157c0ab000-7f157c0d9000 r--p 0018b000 00:24 40382126                   /usr/lib64/libnvidia-ml.so.535.104.12
7f157c0d9000-7f157c0db000 rw-p 001b9000 00:24 40382126                   /usr/lib64/libnvidia-ml.so.535.104.12
7f157c0db000-7f157cccb000 rw-p 00000000 00:00 0 
7f157ccd2000-7f157ccd4000 r--p 00000000 00:2b 250709987                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-x86_64-linux-gnu.so
7f157ccd4000-7f157ccd6000 r-xp 00002000 00:2b 250709987                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-x86_64-linux-gnu.so
7f157ccd6000-7f157ccd7000 r--p 00004000 00:2b 250709987                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-x86_64-linux-gnu.so
7f157ccd7000-7f157ccd8000 ---p 00005000 00:2b 250709987                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-x86_64-linux-gnu.so
7f157ccd8000-7f157ccd9000 r--p 00005000 00:2b 250709987                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-x86_64-linux-gnu.so
7f157ccd9000-7f157ccda000 rw-p 00006000 00:2b 250709987                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-x86_64-linux-gnu.so
7f157ccda000-7f157ccea000 -w-s 00000000 00:05 39720                      /dev/nvidia0
7f157ccea000-7f157ce2a000 rw-p 00000000 00:00 0 
7f157ce2a000-7f157ce31000 r--p 00000000 00:2b 250710026                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so
7f157ce31000-7f157ce63000 r-xp 00007000 00:2b 250710026                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so
7f157ce63000-7f157ce71000 r--p 00039000 00:2b 250710026                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so
7f157ce71000-7f157ce74000 r--p 00046000 00:2b 250710026                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so
7f157ce74000-7f157ce76000 rw-p 00049000 00:2b 250710026                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so
7f157ce76000-7f157ce7a000 r--p 00000000 00:2b 250709983                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so
7f157ce7a000-7f157ce83000 r-xp 00004000 00:2b 250709983                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so
7f157ce83000-7f157ce87000 r--p 0000d000 00:2b 250709983                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so
7f157ce87000-7f157ce88000 r--p 00010000 00:2b 250709983                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so
7f157ce88000-7f157ce8a000 rw-p 00011000 00:2b 250709983                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so
7f157ce8a000-7f157cf0a000 rw-p 00000000 00:00 0 
7f157cf0a000-7f157cf0c000 r--p 00000000 00:2b 250709976                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
7f157cf0c000-7f157cf10000 r-xp 00002000 00:2b 250709976                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
7f157cf10000-7f157cf13000 r--p 00006000 00:2b 250709976                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
7f157cf13000-7f157cf14000 r--p 00008000 00:2b 250709976                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
7f157cf14000-7f157cf15000 rw-p 00009000 00:2b 250709976                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
7f157cf15000-7f157cfd5000 rw-p 00000000 00:00 0 
7f157cfd5000-7f157cfd8000 r--p 00000000 00:2b 250709990                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multibytecodec.cpython-38-x86_64-linux-gnu.so
7f157cfd8000-7f157cfdf000 r-xp 00003000 00:2b 250709990                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multibytecodec.cpython-38-x86_64-linux-gnu.so
7f157cfdf000-7f157cfe1000 r--p 0000a000 00:2b 250709990                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multibytecodec.cpython-38-x86_64-linux-gnu.so
7f157cfe1000-7f157cfe2000 ---p 0000c000 00:2b 250709990                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multibytecodec.cpython-38-x86_64-linux-gnu.so
7f157cfe2000-7f157cfe3000 r--p 0000c000 00:2b 250709990                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multibytecodec.cpython-38-x86_64-linux-gnu.so
7f157cfe3000-7f157cfe4000 rw-p 0000d000 00:2b 250709990                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multibytecodec.cpython-38-x86_64-linux-gnu.so
7f157cfe4000-7f157cff1000 r--p 00000000 00:4001 9278019355868004353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md__mypyc.cpython-38-x86_64-linux-gnu.so
7f157cff1000-7f157d00d000 r-xp 0000d000 00:4001 9278019355868004353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md__mypyc.cpython-38-x86_64-linux-gnu.so
7f157d00d000-7f157d016000 r--p 00029000 00:4001 9278019355868004353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md__mypyc.cpython-38-x86_64-linux-gnu.so
7f157d016000-7f157d017000 ---p 00032000 00:4001 9278019355868004353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md__mypyc.cpython-38-x86_64-linux-gnu.so
7f157d017000-7f157d018000 r--p 00032000 00:4001 9278019355868004353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md__mypyc.cpython-38-x86_64-linux-gnu.so
7f157d018000-7f157d01d000 rw-p 00033000 00:4001 9278019355868004353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md__mypyc.cpython-38-x86_64-linux-gnu.so
7f157d01d000-7f157d01e000 r--p 00000000 00:4001 1634061961064873985      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md.cpython-38-x86_64-linux-gnu.so
7f157d01e000-7f157d01f000 r-xp 00001000 00:4001 1634061961064873985      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md.cpython-38-x86_64-linux-gnu.so
7f157d01f000-7f157d020000 r--p 00002000 00:4001 1634061961064873985      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md.cpython-38-x86_64-linux-gnu.so
7f157d020000-7f157d021000 r--p 00002000 00:4001 1634061961064873985      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md.cpython-38-x86_64-linux-gnu.so
7f157d021000-7f157d022000 rw-p 00003000 00:4001 1634061961064873985      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/charset_normalizer/md.cpython-38-x86_64-linux-gnu.so
7f157d022000-7f157dae2000 rw-p 00000000 00:00 0 
7f157dae2000-7f157dae6000 r--p 00000000 00:2b 250709960                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
7f157dae6000-7f157daf0000 r-xp 00004000 00:2b 250709960                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
7f157daf0000-7f157daf4000 r--p 0000e000 00:2b 250709960                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
7f157daf4000-7f157daf5000 ---p 00012000 00:2b 250709960                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
7f157daf5000-7f157daf6000 r--p 00012000 00:2b 250709960                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
7f157daf6000-7f157daf8000 rw-p 00013000 00:2b 250709960                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
7f157daf8000-7f157ebb9000 rw-p 00000000 00:00 0 
7f157ebb9000-7f157ebc0000 r--p 00000000 00:2b 250709982                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so
7f157ebc0000-7f157ebf5000 r-xp 00007000 00:2b 250709982                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so
7f157ebf5000-7f157ec05000 r--p 0003c000 00:2b 250709982                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so
7f157ec05000-7f157ec06000 r--p 0004b000 00:2b 250709982                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so
7f157ec06000-7f157ec09000 rw-p 0004c000 00:2b 250709982                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so
7f157ec09000-7f157f289000 rw-p 00000000 00:00 0 
7f157f289000-7f157f28b000 r--p 00000000 00:2b 250709996                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so
7f157f28b000-7f157f28c000 r-xp 00002000 00:2b 250709996                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so
7f157f28c000-7f157f28d000 r--p 00003000 00:2b 250709996                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so
7f157f28d000-7f157f28e000 r--p 00003000 00:2b 250709996                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so
7f157f28e000-7f157f28f000 rw-p 00004000 00:2b 250709996                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so
7f157f28f000-7f157f3cf000 rw-p 00000000 00:00 0 
7f157f3cf000-7f157f3d2000 r--p 00000000 00:2b 250710033                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so
7f157f3d2000-7f157f3d7000 r-xp 00003000 00:2b 250710033                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so
7f157f3d7000-7f157f4d9000 r--p 00008000 00:2b 250710033                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so
7f157f4d9000-7f157f4da000 r--p 00109000 00:2b 250710033                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so
7f157f4da000-7f157f4db000 rw-p 0010a000 00:2b 250710033                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so
7f157f4db000-7f157f55b000 rw-p 00000000 00:00 0 
7f157f55b000-7f157f57a000 r--p 00000000 00:2b 250141565                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libssl.so.1.1
7f157f57a000-7f157f5c4000 r-xp 0001f000 00:2b 250141565                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libssl.so.1.1
7f157f5c4000-7f157f5de000 r--p 00069000 00:2b 250141565                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libssl.so.1.1
7f157f5de000-7f157f5e7000 r--p 00082000 00:2b 250141565                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libssl.so.1.1
7f157f5e7000-7f157f5eb000 rw-p 0008b000 00:2b 250141565                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libssl.so.1.1
7f157f5eb000-7f157f5fb000 r--p 00000000 00:2b 250710004                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
7f157f5fb000-7f157f608000 r-xp 00010000 00:2b 250710004                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
7f157f608000-7f157f614000 r--p 0001d000 00:2b 250710004                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
7f157f614000-7f157f615000 ---p 00029000 00:2b 250710004                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
7f157f615000-7f157f616000 r--p 00029000 00:2b 250710004                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
7f157f616000-7f157f61d000 rw-p 0002a000 00:2b 250710004                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
7f157f61d000-7f157f81d000 rw-p 00000000 00:00 0 
7f157f81d000-7f157f821000 r--p 00000000 00:2b 250710015                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so
7f157f821000-7f157f828000 r-xp 00004000 00:2b 250710015                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so
7f157f828000-7f157f82d000 r--p 0000b000 00:2b 250710015                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so
7f157f82d000-7f157f82e000 r--p 0000f000 00:2b 250710015                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so
7f157f82e000-7f157f82f000 rw-p 00010000 00:2b 250710015                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so
7f157f82f000-7f157f834000 r--p 00000000 00:2b 250710002                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
7f157f834000-7f157f842000 r-xp 00005000 00:2b 250710002                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
7f157f842000-7f157f84b000 r--p 00013000 00:2b 250710002                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
7f157f84b000-7f157f84c000 ---p 0001c000 00:2b 250710002                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
7f157f84c000-7f157f84d000 r--p 0001c000 00:2b 250710002                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
7f157f84d000-7f157f84e000 rw-p 0001d000 00:2b 250710002                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
7f157f84e000-7f157fb0e000 rw-p 00000000 00:00 0 
7f157fb4e000-7f157fbcf000 rw-p 00000000 00:00 0 
7f157fbcf000-7f157fd0f000 rw-p 00000000 00:00 0 
7f157fd0f000-7f157fd11000 r--p 00000000 00:2b 250710018                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/cmath.cpython-38-x86_64-linux-gnu.so
7f157fd11000-7f157fd1e000 r-xp 00002000 00:2b 250710018                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/cmath.cpython-38-x86_64-linux-gnu.so
7f157fd1e000-7f157fd20000 r--p 0000f000 00:2b 250710018                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/cmath.cpython-38-x86_64-linux-gnu.so
7f157fd20000-7f157fd21000 r--p 00010000 00:2b 250710018                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/cmath.cpython-38-x86_64-linux-gnu.so
7f157fd21000-7f157fd22000 rw-p 00011000 00:2b 250710018                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/cmath.cpython-38-x86_64-linux-gnu.so
7f157fd22000-7f1580364000 rw-p 00000000 00:00 0 
7f1580364000-7f1580371000 r--p 00000000 00:4001 16706682488907694081     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
7f1580371000-7f15803e8000 r-xp 0000d000 00:4001 16706682488907694081     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
7f15803e8000-7f1580419000 r--p 00084000 00:4001 16706682488907694081     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
7f1580419000-7f158041a000 r--p 000b4000 00:4001 16706682488907694081     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
7f158041a000-7f1580441000 rw-p 000b5000 00:4001 16706682488907694081     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
7f1580441000-7f1580443000 rw-p 00000000 00:00 0 
7f1580443000-7f1580446000 r--p 00000000 00:4001 5299158348105973761      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
7f1580446000-7f158044e000 r-xp 00003000 00:4001 5299158348105973761      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
7f158044e000-7f1580450000 r--p 0000b000 00:4001 5299158348105973761      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
7f1580450000-7f1580451000 ---p 0000d000 00:4001 5299158348105973761      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
7f1580451000-7f1580452000 r--p 0000d000 00:4001 5299158348105973761      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
7f1580452000-7f1580453000 rw-p 0000e000 00:4001 5299158348105973761      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
7f1580453000-7f1580457000 r--p 00000000 00:4001 11730881352222572545     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
7f1580457000-7f1580467000 r-xp 00004000 00:4001 11730881352222572545     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
7f1580467000-7f158046b000 r--p 00014000 00:4001 11730881352222572545     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
7f158046b000-7f158046c000 r--p 00017000 00:4001 11730881352222572545     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
7f158046c000-7f158046f000 rw-p 00018000 00:4001 11730881352222572545     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
7f158046f000-7f1580472000 r--p 00000000 00:4001 18390526679976706049     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
7f1580472000-7f158047f000 r-xp 00003000 00:4001 18390526679976706049     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
7f158047f000-7f1580483000 r--p 00010000 00:4001 18390526679976706049     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
7f1580483000-7f1580484000 r--p 00013000 00:4001 18390526679976706049     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
7f1580484000-7f1580486000 rw-p 00014000 00:4001 18390526679976706049     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
7f1580486000-7f15804c6000 rw-p 00000000 00:00 0 
7f15804c6000-7f15804ca000 r--p 00000000 00:4001 4484274967829217281      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
7f15804ca000-7f15804d9000 r-xp 00004000 00:4001 4484274967829217281      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
7f15804d9000-7f15804de000 r--p 00013000 00:4001 4484274967829217281      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
7f15804de000-7f15804df000 r--p 00017000 00:4001 4484274967829217281      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
7f15804df000-7f15804e1000 rw-p 00018000 00:4001 4484274967829217281      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
7f15804e1000-7f15804e6000 r--p 00000000 00:4001 1278582734881226753      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
7f15804e6000-7f1580530000 r-xp 00005000 00:4001 1278582734881226753      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
7f1580530000-7f1580539000 r--p 0004f000 00:4001 1278582734881226753      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
7f1580539000-7f158053a000 r--p 00057000 00:4001 1278582734881226753      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
7f158053a000-7f158053b000 rw-p 00058000 00:4001 1278582734881226753      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
7f158053b000-7f158053c000 rw-p 00000000 00:00 0 
7f158053c000-7f158053f000 r--p 00000000 00:2b 250710000                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
7f158053f000-7f1580552000 r-xp 00003000 00:2b 250710000                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
7f1580552000-7f1580554000 r--p 00016000 00:2b 250710000                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
7f1580554000-7f1580555000 r--p 00017000 00:2b 250710000                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
7f1580555000-7f1580556000 rw-p 00018000 00:2b 250710000                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
7f1580556000-7f1580558000 r--p 00000000 00:2b 250709966                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
7f1580558000-7f1580563000 r-xp 00002000 00:2b 250709966                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
7f1580563000-7f1580565000 r--p 0000d000 00:2b 250709966                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
7f1580565000-7f1580566000 r--p 0000e000 00:2b 250709966                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
7f1580566000-7f1580567000 rw-p 0000f000 00:2b 250709966                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
7f1580567000-7f15805e2000 r--p 00000000 00:2b 250141350                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libcrypto.so.1.1
7f15805e2000-7f1580774000 r-xp 0007b000 00:2b 250141350                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libcrypto.so.1.1
7f1580774000-7f1580801000 r--p 0020d000 00:2b 250141350                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libcrypto.so.1.1
7f1580801000-7f1580802000 ---p 0029a000 00:2b 250141350                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libcrypto.so.1.1
7f1580802000-7f158082d000 r--p 0029a000 00:2b 250141350                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libcrypto.so.1.1
7f158082d000-7f158082f000 rw-p 002c5000 00:2b 250141350                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libcrypto.so.1.1
7f158082f000-7f1580833000 rw-p 00000000 00:00 0 
7f1580833000-7f1580836000 r--p 00000000 00:2b 250709984                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
7f1580836000-7f158083a000 r-xp 00003000 00:2b 250709984                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
7f158083a000-7f158083c000 r--p 00007000 00:2b 250709984                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
7f158083c000-7f158083d000 ---p 00009000 00:2b 250709984                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
7f158083d000-7f158083e000 r--p 00009000 00:2b 250709984                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
7f158083e000-7f158083f000 rw-p 0000a000 00:2b 250709984                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
7f158083f000-7f1580841000 r--p 00000000 00:2b 250710017                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
7f1580841000-7f1580845000 r-xp 00002000 00:2b 250710017                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
7f1580845000-7f1580847000 r--p 00006000 00:2b 250710017                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
7f1580847000-7f1580848000 ---p 00008000 00:2b 250710017                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
7f1580848000-7f1580849000 r--p 00008000 00:2b 250710017                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
7f1580849000-7f158084a000 rw-p 00009000 00:2b 250710017                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
7f158084a000-7f158084f000 r--p 00000000 00:4001 15334297306784137217     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
7f158084f000-7f1580888000 r-xp 00005000 00:4001 15334297306784137217     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
7f1580888000-7f158088c000 r--p 0003e000 00:4001 15334297306784137217     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
7f158088c000-7f158088d000 ---p 00042000 00:4001 15334297306784137217     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
7f158088d000-7f158088e000 r--p 00042000 00:4001 15334297306784137217     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
7f158088e000-7f1580890000 rw-p 00043000 00:4001 15334297306784137217     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
7f1580890000-7f1580891000 rw-p 00000000 00:00 0 
7f1580891000-7f1580897000 r--p 00000000 00:4001 8195849648624107521      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
7f1580897000-7f15808b2000 r-xp 00006000 00:4001 8195849648624107521      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
7f15808b2000-7f15808ba000 r--p 00021000 00:4001 8195849648624107521      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
7f15808ba000-7f15808bb000 r--p 00028000 00:4001 8195849648624107521      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
7f15808bb000-7f15808c0000 rw-p 00029000 00:4001 8195849648624107521      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
7f15808c0000-7f15808cc000 r--p 00000000 00:4001 6600665067570069505      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
7f15808cc000-7f158091b000 r-xp 0000c000 00:4001 6600665067570069505      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
7f158091b000-7f1580949000 r--p 0005b000 00:4001 6600665067570069505      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
7f1580949000-7f158094a000 r--p 00088000 00:4001 6600665067570069505      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
7f158094a000-7f1580972000 rw-p 00089000 00:4001 6600665067570069505      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
7f1580972000-7f15809b4000 rw-p 00000000 00:00 0 
7f15809b4000-7f15809b5000 r--p 00000000 00:4001 13782848918776774657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
7f15809b5000-7f15809c8000 r-xp 00001000 00:4001 13782848918776774657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
7f15809c8000-7f15809ca000 r--p 00014000 00:4001 13782848918776774657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
7f15809ca000-7f15809cb000 r--p 00015000 00:4001 13782848918776774657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
7f15809cb000-7f15809cc000 rw-p 00016000 00:4001 13782848918776774657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
7f15809cc000-7f1580a8c000 rw-p 00000000 00:00 0 
7f1580a8c000-7f1580a93000 r--p 00000000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580a93000-7f1580ab4000 r-xp 00007000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580ab4000-7f1580ab9000 r--p 00028000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580ab9000-7f1580aba000 ---p 0002d000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580aba000-7f1580abb000 r--p 0002d000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580abb000-7f1580abc000 rw-p 0002e000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580abc000-7f1580abd000 rw-p 00000000 00:00 0 
7f1580abd000-7f1580ac1000 rw-p 00035000 00:4001 10682706869826551809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
7f1580ac1000-7f1580b41000 rw-p 00000000 00:00 0 
7f1580b41000-7f1580b48000 r--p 00000000 00:4001 16346837759587975169     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
7f1580b48000-7f1580b5f000 r-xp 00007000 00:4001 16346837759587975169     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
7f1580b5f000-7f1580b65000 r--p 0001e000 00:4001 16346837759587975169     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
7f1580b65000-7f1580b66000 r--p 00023000 00:4001 16346837759587975169     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
7f1580b66000-7f1580b68000 rw-p 00024000 00:4001 16346837759587975169     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
7f1580b68000-7f1580c68000 rw-p 00000000 00:00 0 
7f1580c68000-7f1580c6d000 r--p 00000000 00:2b 250709993                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
7f1580c6d000-7f1580c83000 r-xp 00005000 00:2b 250709993                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
7f1580c83000-7f1580c89000 r--p 0001b000 00:2b 250709993                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
7f1580c89000-7f1580c8a000 ---p 00021000 00:2b 250709993                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
7f1580c8a000-7f1580c8b000 r--p 00021000 00:2b 250709993                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
7f1580c8b000-7f1580c8d000 rw-p 00022000 00:2b 250709993                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
7f1580c8d000-7f1580d4d000 rw-p 00000000 00:00 0 
7f1580d4d000-7f1580d52000 r--p 00000000 00:2b 250709981                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
7f1580d52000-7f1580d66000 r-xp 00005000 00:2b 250709981                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
7f1580d66000-7f1580d6c000 r--p 00019000 00:2b 250709981                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
7f1580d6c000-7f1580d6d000 r--p 0001e000 00:2b 250709981                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
7f1580d6d000-7f1580d6f000 rw-p 0001f000 00:2b 250709981                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
7f1580d6f000-7f1580daf000 rw-p 00000000 00:00 0 
7f1580daf000-7f1580dea000 r-xp 00000000 00:4001 5019087336067956737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libquadmath-96973f99.so.0.0.0
7f1580dea000-7f1580fe9000 ---p 0003b000 00:4001 5019087336067956737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libquadmath-96973f99.so.0.0.0
7f1580fe9000-7f1580fea000 r--p 0003a000 00:4001 5019087336067956737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libquadmath-96973f99.so.0.0.0
7f1580fea000-7f1580fec000 rw-p 0003b000 00:4001 5019087336067956737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libquadmath-96973f99.so.0.0.0
7f1580fec000-7f1581262000 r-xp 00000000 00:4001 5295251070116102145      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0
7f1581262000-7f1581461000 ---p 00276000 00:4001 5295251070116102145      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0
7f1581461000-7f1581462000 r--p 00275000 00:4001 5295251070116102145      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0
7f1581462000-7f1581464000 rw-p 00276000 00:4001 5295251070116102145      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0
7f1581464000-7f1581479000 rw-p 0027b000 00:4001 5295251070116102145      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0
7f1581479000-7f1581576000 r--p 00000000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f1581576000-7f158157e000 r-xp 000fd000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f158157e000-7f1581581000 ---p 00105000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f1581581000-7f158302c000 r-xp 00108000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f158302c000-7f1583228000 r--p 01bb3000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f1583228000-7f1583229000 ---p 01daf000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f1583229000-7f158322f000 r--p 01daf000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f158322f000-7f1583244000 rw-p 01db5000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f1583244000-7f1583251000 rw-p 00000000 00:00 0 
7f1583251000-7f15832f3000 rw-p 01ecc000 00:4001 14418721289173008385     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
7f15832f3000-7f158331b000 r--p 00000000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f158331b000-7f1583756000 r-xp 00028000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f1583756000-7f158386d000 r--p 00463000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f158386d000-7f158386e000 ---p 0057a000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f158386e000-7f1583870000 r--p 0057a000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f1583870000-7f1583892000 rw-p 0057c000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f1583892000-7f15838b3000 rw-p 00000000 00:00 0 
7f15838b3000-7f15838ba000 rw-p 005fe000 00:4001 8521434489375686657      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
7f15838ba000-7f15838bd000 r--p 00000000 00:2b 250709986                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so
7f15838bd000-7f15838ca000 r-xp 00003000 00:2b 250709986                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so
7f15838ca000-7f15838cc000 r--p 00010000 00:2b 250709986                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so
7f15838cc000-7f15838cd000 r--p 00011000 00:2b 250709986                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so
7f15838cd000-7f15838ce000 rw-p 00012000 00:2b 250709986                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so
7f15838ce000-7f158394e000 rw-p 00000000 00:00 0 
7f158394e000-7f15867a8000 r-xp 00000000 00:4001 16448247481060622337     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12
7f15867a8000-7f15869a8000 ---p 02e5a000 00:4001 16448247481060622337     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12
7f15869a8000-7f15870bd000 r--p 02e5a000 00:4001 16448247481060622337     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12
7f15870bd000-7f1587189000 rw-p 0356f000 00:4001 16448247481060622337     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12
7f1587189000-7f1587285000 rw-p 00000000 00:00 0 
7f1587285000-7f1587287000 rw-p 0363c000 00:4001 16448247481060622337     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12
7f1587287000-7f1587380000 r--p 00000000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f1587380000-7f15876fa000 r-xp 000f9000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f15876fa000-7f1587830000 r--p 00473000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f1587830000-7f1587831000 ---p 005a9000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f1587831000-7f1587849000 r--p 005a9000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f1587849000-7f158784f000 rw-p 005c1000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f158784f000-7f1587850000 rw-p 00000000 00:00 0 
7f1587850000-7f15878d5000 rw-p 0074a000 00:4001 13940839352020893697     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libnvfuser_codegen.so
7f15878d5000-7f158a47e000 r-xp 00000000 00:4001 12053941211222966273     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12
7f158a47e000-7f158a67e000 ---p 02ba9000 00:4001 12053941211222966273     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12
7f158a67e000-7f158ac29000 r--p 02ba9000 00:4001 12053941211222966273     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12
7f158ac29000-7f158ac9b000 rw-p 03154000 00:4001 12053941211222966273     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12
7f158ac9b000-7f158ad11000 rw-p 00000000 00:00 0 
7f158ad11000-7f159acb4000 r-xp 00000000 00:4001 17808382821171986433     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nccl/lib/libnccl.so.2
7f159acb4000-7f159aeb4000 ---p 0ffa3000 00:4001 17808382821171986433     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nccl/lib/libnccl.so.2
7f159aeb4000-7f159aed0000 r--p 0ffa3000 00:4001 17808382821171986433     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nccl/lib/libnccl.so.2
7f159aed0000-7f159aede000 rw-p 0ffbf000 00:4001 17808382821171986433     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nccl/lib/libnccl.so.2
7f159aede000-7f159aefe000 rw-p 00000000 00:00 0 
7f159aefe000-7f15aab6b000 r-xp 00000000 00:4001 13260579503079096321     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cusparse/lib/libcusparse.so.12
7f15aab6b000-7f15aad6b000 ---p 0fc6d000 00:4001 13260579503079096321     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cusparse/lib/libcusparse.so.12
7f15aad6b000-7f15aad7a000 r--p 0fc6d000 00:4001 13260579503079096321     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cusparse/lib/libcusparse.so.12
7f15aad7a000-7f15aad94000 rw-p 0fc7c000 00:4001 13260579503079096321     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cusparse/lib/libcusparse.so.12
7f15aad94000-7f15aad9a000 rw-p 00000000 00:00 0 
7f15aad9a000-7f15aad9f000 rw-p 0fc97000 00:4001 13260579503079096321     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cusparse/lib/libcusparse.so.12
7f15aad9f000-7f15ab3d1000 r-xp 00000000 00:4001 854146450041667585       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_cupti/lib/libcupti.so.12
7f15ab3d1000-7f15ab5d0000 ---p 00632000 00:4001 854146450041667585       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_cupti/lib/libcupti.so.12
7f15ab5d0000-7f15ab61d000 r--p 00631000 00:4001 854146450041667585       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_cupti/lib/libcupti.so.12
7f15ab61d000-7f15ab6b2000 rw-p 0067e000 00:4001 854146450041667585       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_cupti/lib/libcupti.so.12
7f15ab6b2000-7f15ab6ec000 rw-p 00000000 00:00 0 
7f15ab6ec000-7f15ab710000 r-xp 00000000 00:4001 1053629996037570561      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn.so.8
7f15ab710000-7f15ab90f000 ---p 00024000 00:4001 1053629996037570561      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn.so.8
7f15ab90f000-7f15ab910000 r--p 00023000 00:4001 1053629996037570561      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn.so.8
7f15ab910000-7f15ab911000 rw-p 00024000 00:4001 1053629996037570561      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn.so.8
7f15ab911000-7f15ab912000 rw-p 00000000 00:00 0 
7f15ab912000-7f15ab937000 r--p 00000000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab937000-7f15ab9ab000 r-xp 00025000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab9ab000-7f15ab9d1000 r--p 00099000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab9d1000-7f15ab9d2000 ---p 000bf000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab9d2000-7f15ab9d4000 r--p 000bf000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab9d4000-7f15ab9da000 rw-p 000c1000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab9da000-7f15ab9dd000 rw-p 00000000 00:00 0 
7f15ab9dd000-7f15ab9f0000 rw-p 000f9000 00:4001 111932618015899649       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10.so
7f15ab9f0000-7f15aba00000 r--p 00000000 00:4001 3615114009592332289      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so
7f15aba00000-7f15aba51000 r-xp 00010000 00:4001 3615114009592332289      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so
7f15aba51000-7f15aba64000 r--p 00061000 00:4001 3615114009592332289      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so
7f15aba64000-7f15aba65000 r--p 00073000 00:4001 3615114009592332289      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so
7f15aba65000-7f15aba66000 rw-p 00074000 00:4001 3615114009592332289      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so
7f15aba66000-7f15aba6b000 rw-p 00000000 00:00 0 
7f15aba6b000-7f15aba73000 rw-p 00091000 00:4001 3615114009592332289      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so
7f15aba73000-7f15ac6ab000 r--p 00000000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15ac6ab000-7f15aec7f000 r-xp 00c38000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15aec7f000-7f15d5be7000 r--p 0320c000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15d5be7000-7f15d5be8000 ---p 2a174000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15d5be8000-7f15d5c4d000 r--p 2a174000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15d5c4d000-7f15d5c68000 rw-p 2a1d9000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15d5c68000-7f15d6973000 rw-p 00000000 00:00 0 
7f15d6973000-7f15d732e000 rw-p 2b6e4000 00:4001 16809614101764767745     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so
7f15d732e000-7f15d8280000 r--p 00000000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15d8280000-7f15ea4e1000 r-xp 00f52000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15ea4e1000-7f15ed283000 r--p 131b3000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15ed283000-7f15ed284000 ---p 15f55000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15ed284000-7f15ed4b4000 r--p 15f55000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15ed4b4000-7f15ed5ba000 rw-p 16185000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15ed5ba000-7f15ed64a000 rw-p 00000000 00:00 0 
7f15ed64a000-7f15ee247000 rw-p 1b145000 00:4001 2098895331103604737      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so
7f15ee247000-7f15ee253000 r--p 00000000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee253000-7f15ee25e000 r-xp 0000c000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee25e000-7f15ee263000 r--p 00017000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee263000-7f15ee264000 ---p 0001c000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee264000-7f15ee265000 r--p 0001c000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee265000-7f15ee266000 rw-p 0001d000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee266000-7f15ee26d000 rw-p 00029000 00:4001 4993613409925988353      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch.so
7f15ee26d000-7f15ee465000 r--p 00000000 00:4001 7734711487430983681      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so
7f15ee465000-7f15eef2e000 r-xp 001f8000 00:4001 7734711487430983681      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so
7f15eef2e000-7f15ef26a000 r--p 00cc1000 00:4001 7734711487430983681      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so
7f15ef26a000-7f15ef286000 r--p 00ffc000 00:4001 7734711487430983681      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so
7f15ef286000-7f15ef2d9000 rw-p 01018000 00:4001 7734711487430983681      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so
7f15ef2d9000-7f15ef33c000 rw-p 00000000 00:00 0 
7f15ef33c000-7f15ef43e000 rw-p 0161b000 00:4001 7734711487430983681      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so
7f15ef43e000-7f15ef51f000 r--p 00000000 00:24 40382112                   /usr/lib64/libcuda.so.535.104.12
7f15ef51f000-7f15efa37000 r-xp 000e1000 00:24 40382112                   /usr/lib64/libcuda.so.535.104.12
7f15efa37000-7f15f0f15000 r--p 005f9000 00:24 40382112                   /usr/lib64/libcuda.so.535.104.12
7f15f0f15000-7f15f0f2e000 r--p 01ad6000 00:24 40382112                   /usr/lib64/libcuda.so.535.104.12
7f15f0f2e000-7f15f1034000 rw-p 01aef000 00:24 40382112                   /usr/lib64/libcuda.so.535.104.12
7f15f1034000-7f15f109b000 rw-p 00000000 00:00 0 
7f15f109b000-7f15f113e000 r--p 00000000 00:2b 250141566                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libstdc++.so.6.0.28
7f15f113e000-7f15f11bd000 r-xp 000a3000 00:2b 250141566                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libstdc++.so.6.0.28
7f15f11bd000-7f15f11fe000 r--p 00122000 00:2b 250141566                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libstdc++.so.6.0.28
7f15f11fe000-7f15f1209000 r--p 00162000 00:2b 250141566                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libstdc++.so.6.0.28
7f15f1209000-7f15f120d000 rw-p 0016d000 00:2b 250141566                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libstdc++.so.6.0.28
7f15f120d000-7f15f1210000 rw-p 00000000 00:00 0 
7f15f1210000-7f15f1225000 r-xp 00000000 00:24 40385453                   /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7f15f1225000-7f15f1424000 ---p 00015000 00:24 40385453                   /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7f15f1424000-7f15f1425000 r--p 00014000 00:24 40385453                   /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7f15f1425000-7f15f1426000 rw-p 00015000 00:24 40385453                   /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7f15f1426000-7f15f1427000 -w-s 00900000 00:05 47135                      /dev/infiniband/uverbs0
7f15f1427000-7f15f1428000 r--s 00700000 00:05 47135                      /dev/infiniband/uverbs0
7f15f1428000-7f15f1429000 r--s 00500000 00:05 47135                      /dev/infiniband/uverbs0
7f15f1429000-7f15f142b000 r--p 00000000 00:2b 250709991                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so
7f15f142b000-7f15f142c000 r-xp 00002000 00:2b 250709991                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so
7f15f142c000-7f15f142d000 r--p 00003000 00:2b 250709991                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so
7f15f142d000-7f15f142e000 r--p 00003000 00:2b 250709991                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so
7f15f142e000-7f15f142f000 rw-p 00004000 00:2b 250709991                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so
7f15f142f000-7f15f1430000 r--p 00000000 00:2b 250709974                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so
7f15f1430000-7f15f1431000 r-xp 00001000 00:2b 250709974                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so
7f15f1431000-7f15f1432000 r--p 00002000 00:2b 250709974                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so
7f15f1432000-7f15f1433000 r--p 00002000 00:2b 250709974                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so
7f15f1433000-7f15f1434000 rw-p 00003000 00:2b 250709974                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so
7f15f1434000-7f15f1437000 r--p 00000000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f1437000-7f15f143a000 r-xp 00003000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f143a000-7f15f143b000 r--p 00006000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f143b000-7f15f143c000 ---p 00007000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f143c000-7f15f143d000 r--p 00007000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f143d000-7f15f143e000 rw-p 00008000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f143e000-7f15f143f000 rw-p 0000b000 00:4001 4366664732466675713      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libshm.so
7f15f143f000-7f15f1440000 r--p 00000000 00:4001 4121192085288845313      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_C.cpython-38-x86_64-linux-gnu.so
7f15f1440000-7f15f1441000 r-xp 00001000 00:4001 4121192085288845313      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_C.cpython-38-x86_64-linux-gnu.so
7f15f1441000-7f15f1442000 r--p 00002000 00:4001 4121192085288845313      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_C.cpython-38-x86_64-linux-gnu.so
7f15f1442000-7f15f1443000 r--p 00002000 00:4001 4121192085288845313      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_C.cpython-38-x86_64-linux-gnu.so
7f15f1443000-7f15f1444000 rw-p 00003000 00:4001 4121192085288845313      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_C.cpython-38-x86_64-linux-gnu.so
7f15f1444000-7f15f1445000 rw-p 00009000 00:4001 4121192085288845313      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_C.cpython-38-x86_64-linux-gnu.so
7f15f1445000-7f15f146a000 r-xp 00000000 00:4001 5081772100249649153      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libgomp-a34b3233.so.1
7f15f146a000-7f15f1669000 ---p 00025000 00:4001 5081772100249649153      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libgomp-a34b3233.so.1
7f15f1669000-7f15f166a000 r--p 00024000 00:4001 5081772100249649153      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libgomp-a34b3233.so.1
7f15f166a000-7f15f166f000 rw-p 00025000 00:4001 5081772100249649153      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libgomp-a34b3233.so.1
7f15f166f000-7f15f1677000 r-xp 00000000 00:4001 268717988430807041       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvtx/lib/libnvToolsExt.so.1
7f15f1677000-7f15f1877000 ---p 00008000 00:4001 268717988430807041       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvtx/lib/libnvToolsExt.so.1
7f15f1877000-7f15f1878000 r--p 00008000 00:4001 268717988430807041       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvtx/lib/libnvToolsExt.so.1
7f15f1878000-7f15f1879000 rw-p 00009000 00:4001 268717988430807041       /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/nvtx/lib/libnvToolsExt.so.1
7f15f1879000-7f15f1919000 r-xp 00000000 00:4001 13579642472377286657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_runtime/lib/libcudart.so.12
7f15f1919000-7f15f1b19000 ---p 000a0000 00:4001 13579642472377286657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_runtime/lib/libcudart.so.12
7f15f1b19000-7f15f1b1e000 r--p 000a0000 00:4001 13579642472377286657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_runtime/lib/libcudart.so.12
7f15f1b1e000-7f15f1b1f000 rw-p 000a5000 00:4001 13579642472377286657     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cuda_runtime/lib/libcudart.so.12
7f15f1b1f000-7f15f1b20000 rw-p 00000000 00:00 0 
7f15f1b20000-7f160f4ae000 r-xp 00000000 00:4001 10810703942449299457     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublasLt.so.12
7f160f4ae000-7f160f6ae000 ---p 1d98e000 00:4001 10810703942449299457     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublasLt.so.12
7f160f6ae000-7f160f86a000 r--p 1d98e000 00:4001 10810703942449299457     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublasLt.so.12
7f160f86a000-7f161085a000 rw-p 1db4a000 00:4001 10810703942449299457     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublasLt.so.12
7f161085a000-7f1613aea000 rw-p 00000000 00:00 0 
7f1613aea000-7f161a15b000 r-xp 00000000 00:4001 11569483014599671809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublas.so.12
7f161a15b000-7f161a35b000 ---p 06671000 00:4001 11569483014599671809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublas.so.12
7f161a35b000-7f161a362000 r--p 06671000 00:4001 11569483014599671809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublas.so.12
7f161a362000-7f161a369000 rw-p 06678000 00:4001 11569483014599671809     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cublas/lib/libcublas.so.12
7f161a369000-7f161a371000 rw-p 00000000 00:00 0 
7f161a371000-7f161eb73000 r-xp 00000000 00:4001 9117468769970552833      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/curand/lib/libcurand.so.10
7f161eb73000-7f161ed73000 ---p 04802000 00:4001 9117468769970552833      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/curand/lib/libcurand.so.10
7f161ed73000-7f161ed79000 r--p 04802000 00:4001 9117468769970552833      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/curand/lib/libcurand.so.10
7f161ed79000-7f16201a5000 rw-p 04808000 00:4001 9117468769970552833      /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/curand/lib/libcurand.so.10
7f16201a5000-7f16207dd000 rw-p 00000000 00:00 0 
7f16207dd000-7f16245a3000 r-xp 00000000 00:4001 11788887232743997441     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cufft/lib/libcufft.so.11
7f16245a3000-7f16247a3000 ---p 03dc6000 00:4001 11788887232743997441     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cufft/lib/libcufft.so.11
7f16247a3000-7f1624802000 r--p 03dc6000 00:4001 11788887232743997441     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cufft/lib/libcufft.so.11
7f1624802000-7f162c1a6000 rw-p 03e25000 00:4001 11788887232743997441     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/nvidia/cufft/lib/libcufft.so.11
7f162c1a6000-7f162c297000 rw-p 00000000 00:00 0 
7f162c297000-7f162c298000 r--p 00000000 00:4001 15172332947170918401     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so
7f162c298000-7f162c299000 r-xp 00001000 00:4001 15172332947170918401     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so
7f162c299000-7f162c29a000 r--p 00002000 00:4001 15172332947170918401     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so
7f162c29a000-7f162c29b000 r--p 00002000 00:4001 15172332947170918401     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so
7f162c29b000-7f162c29e000 rw-p 00003000 00:4001 15172332947170918401     /sfs/weka/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so
7f162c29e000-7f162c2a1000 r--p 00000000 00:2b 250710029                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
7f162c2a1000-7f162c2a5000 r-xp 00003000 00:2b 250710029                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
7f162c2a5000-7f162c2a7000 r--p 00007000 00:2b 250710029                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
7f162c2a7000-7f162c2a8000 ---p 00009000 00:2b 250710029                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
7f162c2a8000-7f162c2a9000 r--p 00009000 00:2b 250710029                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
7f162c2a9000-7f162c2aa000 rw-p 0000a000 00:2b 250710029                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
7f162c2aa000-7f162c2ac000 r--p 00000000 00:2b 250709995                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so
7f162c2ac000-7f162c2ae000 r-xp 00002000 00:2b 250709995                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so
7f162c2ae000-7f162c2af000 r--p 00004000 00:2b 250709995                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so
7f162c2af000-7f162c2b0000 r--p 00004000 00:2b 250709995                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so
7f162c2b0000-7f162c2b1000 rw-p 00005000 00:2b 250709995                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so
7f162c2b1000-7f162c371000 rw-p 00000000 00:00 0 
7f162c371000-7f162c372000 r--p 00000000 00:2b 250709992                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
7f162c372000-7f162c373000 r-xp 00001000 00:2b 250709992                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
7f162c373000-7f162c374000 r--p 00002000 00:2b 250709992                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
7f162c374000-7f162c375000 r--p 00002000 00:2b 250709992                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
7f162c375000-7f162c376000 rw-p 00003000 00:2b 250709992                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
7f162c376000-7f162c3b6000 rw-p 00000000 00:00 0 
7f162c3b6000-7f162c3b9000 r--p 00000000 00:2b 250710006                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
7f162c3b9000-7f162c3be000 r-xp 00003000 00:2b 250710006                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
7f162c3be000-7f162c3c2000 r--p 00008000 00:2b 250710006                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
7f162c3c2000-7f162c3c3000 r--p 0000b000 00:2b 250710006                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
7f162c3c3000-7f162c3c4000 rw-p 0000c000 00:2b 250710006                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
7f162c3c4000-7f162c3c6000 r--p 00000000 00:2b 250141360                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libffi.so.7.1.0
7f162c3c6000-7f162c3cc000 r-xp 00002000 00:2b 250141360                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libffi.so.7.1.0
7f162c3cc000-7f162c3cd000 r--p 00008000 00:2b 250141360                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libffi.so.7.1.0
7f162c3cd000-7f162c3ce000 ---p 00009000 00:2b 250141360                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libffi.so.7.1.0
7f162c3ce000-7f162c3cf000 r--p 00009000 00:2b 250141360                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libffi.so.7.1.0
7f162c3cf000-7f162c3d0000 rw-p 0000a000 00:2b 250141360                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libffi.so.7.1.0
7f162c3d0000-7f162c3d8000 r--p 00000000 00:2b 250709977                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
7f162c3d8000-7f162c3e7000 r-xp 00008000 00:2b 250709977                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
7f162c3e7000-7f162c3ee000 r--p 00017000 00:2b 250709977                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
7f162c3ee000-7f162c3ef000 r--p 0001d000 00:2b 250709977                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
7f162c3ef000-7f162c3f3000 rw-p 0001e000 00:2b 250709977                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
7f162c3f3000-7f162c433000 rw-p 00000000 00:00 0 
7f162c433000-7f162c435000 r--p 00000000 00:2b 250709997                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
7f162c435000-7f162c437000 r-xp 00002000 00:2b 250709997                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
7f162c437000-7f162c438000 r--p 00004000 00:2b 250709997                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
7f162c438000-7f162c439000 r--p 00004000 00:2b 250709997                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
7f162c439000-7f162c43a000 rw-p 00005000 00:2b 250709997                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
7f162c43a000-7f162c43b000 r--p 00000000 00:2b 250710001                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
7f162c43b000-7f162c442000 r-xp 00001000 00:2b 250710001                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
7f162c442000-7f162c443000 r--p 00008000 00:2b 250710001                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
7f162c443000-7f162c444000 r--p 00008000 00:2b 250710001                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
7f162c444000-7f162c445000 rw-p 00009000 00:2b 250710001                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
7f162c445000-7f162c446000 r--p 00000000 00:2b 250709965                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
7f162c446000-7f162c447000 r-xp 00001000 00:2b 250709965                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
7f162c447000-7f162c448000 r--p 00002000 00:2b 250709965                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
7f162c448000-7f162c449000 r--p 00002000 00:2b 250709965                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
7f162c449000-7f162c44a000 rw-p 00003000 00:2b 250709965                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
7f162c44a000-7f162c44e000 r--p 00000000 00:2b 250710021                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
7f162c44e000-7f162c455000 r-xp 00004000 00:2b 250710021                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
7f162c455000-7f162c459000 r--p 0000b000 00:2b 250710021                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
7f162c459000-7f162c45a000 r--p 0000e000 00:2b 250710021                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
7f162c45a000-7f162c45b000 rw-p 0000f000 00:2b 250710021                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
7f162c45b000-7f162c51b000 rw-p 00000000 00:00 0 
7f162c51b000-7f162c51d000 r--p 00000000 00:2b 250710020                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
7f162c51d000-7f162c51e000 r-xp 00002000 00:2b 250710020                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
7f162c51e000-7f162c51f000 r--p 00003000 00:2b 250710020                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
7f162c51f000-7f162c520000 r--p 00003000 00:2b 250710020                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
7f162c520000-7f162c521000 rw-p 00004000 00:2b 250710020                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
7f162c521000-7f162c525000 r--p 00000000 00:2b 250141479                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/liblzma.so.5.2.5
7f162c525000-7f162c53c000 r-xp 00004000 00:2b 250141479                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/liblzma.so.5.2.5
7f162c53c000-7f162c547000 r--p 0001b000 00:2b 250141479                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/liblzma.so.5.2.5
7f162c547000-7f162c548000 ---p 00026000 00:2b 250141479                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/liblzma.so.5.2.5
7f162c548000-7f162c549000 r--p 00026000 00:2b 250141479                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/liblzma.so.5.2.5
7f162c549000-7f162c54a000 rw-p 00027000 00:2b 250141479                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/liblzma.so.5.2.5
7f162c54a000-7f162c54d000 r--p 00000000 00:2b 250709988                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
7f162c54d000-7f162c550000 r-xp 00003000 00:2b 250709988                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
7f162c550000-7f162c553000 r--p 00006000 00:2b 250709988                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
7f162c553000-7f162c554000 r--p 00008000 00:2b 250709988                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
7f162c554000-7f162c555000 rw-p 00009000 00:2b 250709988                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
7f162c555000-7f162c595000 rw-p 00000000 00:00 0 
7f162c595000-7f162c598000 r--p 00000000 00:2b 250141640                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libz.so.1.2.11
7f162c598000-7f162c5ac000 r-xp 00003000 00:2b 250141640                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libz.so.1.2.11
7f162c5ac000-7f162c5b3000 r--p 00017000 00:2b 250141640                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libz.so.1.2.11
7f162c5b3000-7f162c5b4000 r--p 0001d000 00:2b 250141640                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libz.so.1.2.11
7f162c5b4000-7f162c5b5000 rw-p 0001e000 00:2b 250141640                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/libz.so.1.2.11
7f162c5b5000-7f162c635000 rw-p 00000000 00:00 0 
7f162c635000-7f1632b78000 r--p 00000000 00:24 40448227                   /usr/lib/locale/locale-archive
7f1632b78000-7f1632d3c000 r-xp 00000000 00:24 40384110                   /usr/lib64/libc-2.17.so
7f1632d3c000-7f1632f3b000 ---p 001c4000 00:24 40384110                   /usr/lib64/libc-2.17.so
7f1632f3b000-7f1632f3f000 r--p 001c3000 00:24 40384110                   /usr/lib64/libc-2.17.so
7f1632f3f000-7f1632f41000 rw-p 001c7000 00:24 40384110                   /usr/lib64/libc-2.17.so
7f1632f41000-7f1632f46000 rw-p 00000000 00:00 0 
7f1632f46000-7f1633047000 r-xp 00000000 00:24 40385736                   /usr/lib64/libm-2.17.so
7f1633047000-7f1633246000 ---p 00101000 00:24 40385736                   /usr/lib64/libm-2.17.so
7f1633246000-7f1633247000 r--p 00100000 00:24 40385736                   /usr/lib64/libm-2.17.so
7f1633247000-7f1633248000 rw-p 00101000 00:24 40385736                   /usr/lib64/libm-2.17.so
7f1633248000-7f163324f000 r-xp 00000000 00:24 40386029                   /usr/lib64/librt-2.17.so
7f163324f000-7f163344e000 ---p 00007000 00:24 40386029                   /usr/lib64/librt-2.17.so
7f163344e000-7f163344f000 r--p 00006000 00:24 40386029                   /usr/lib64/librt-2.17.so
7f163344f000-7f1633450000 rw-p 00007000 00:24 40386029                   /usr/lib64/librt-2.17.so
7f1633450000-7f1633452000 r-xp 00000000 00:24 40386184                   /usr/lib64/libutil-2.17.so
7f1633452000-7f1633651000 ---p 00002000 00:24 40386184                   /usr/lib64/libutil-2.17.so
7f1633651000-7f1633652000 r--p 00001000 00:24 40386184                   /usr/lib64/libutil-2.17.so
7f1633652000-7f1633653000 rw-p 00002000 00:24 40386184                   /usr/lib64/libutil-2.17.so
7f1633653000-7f1633655000 r-xp 00000000 00:24 40384215                   /usr/lib64/libdl-2.17.so
7f1633655000-7f1633855000 ---p 00002000 00:24 40384215                   /usr/lib64/libdl-2.17.so
7f1633855000-7f1633856000 r--p 00002000 00:24 40384215                   /usr/lib64/libdl-2.17.so
7f1633856000-7f1633857000 rw-p 00003000 00:24 40384215                   /usr/lib64/libdl-2.17.so
7f1633857000-7f163386e000 r-xp 00000000 00:24 40385971                   /usr/lib64/libpthread-2.17.so
7f163386e000-7f1633a6d000 ---p 00017000 00:24 40385971                   /usr/lib64/libpthread-2.17.so
7f1633a6d000-7f1633a6e000 r--p 00016000 00:24 40385971                   /usr/lib64/libpthread-2.17.so
7f1633a6e000-7f1633a6f000 rw-p 00017000 00:24 40385971                   /usr/lib64/libpthread-2.17.so
7f1633a6f000-7f1633a73000 rw-p 00000000 00:00 0 
7f1633a73000-7f1633a95000 r-xp 00000000 00:24 40383882                   /usr/lib64/ld-2.17.so
7f1633a95000-7f1633a96000 r--s 00000000 00:05 39723                      /dev/nvidia3
7f1633a96000-7f1633a99000 r--p 00000000 00:2b 250709967                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
7f1633a99000-7f1633aa9000 r-xp 00003000 00:2b 250709967                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
7f1633aa9000-7f1633aac000 r--p 00013000 00:2b 250709967                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
7f1633aac000-7f1633aad000 r--p 00015000 00:2b 250709967                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
7f1633aad000-7f1633aaf000 rw-p 00016000 00:2b 250709967                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
7f1633aaf000-7f1633c74000 rw-p 00000000 00:00 0 
7f1633c74000-7f1633c75000 r--s 00000000 00:05 39722                      /dev/nvidia2
7f1633c75000-7f1633c76000 r--s 00000000 00:05 39721                      /dev/nvidia1
7f1633c76000-7f1633c77000 r--s 00000000 00:05 39720                      /dev/nvidia0
7f1633c77000-7f1633c79000 r--p 00000000 00:2b 250710035                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
7f1633c79000-7f1633c7e000 r-xp 00002000 00:2b 250710035                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
7f1633c7e000-7f1633c81000 r--p 00007000 00:2b 250710035                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
7f1633c81000-7f1633c82000 r--p 00009000 00:2b 250710035                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
7f1633c82000-7f1633c83000 rw-p 0000a000 00:2b 250710035                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
7f1633c83000-7f1633c84000 r--p 00000000 00:2b 250709985                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
7f1633c84000-7f1633c87000 r-xp 00001000 00:2b 250709985                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
7f1633c87000-7f1633c8a000 r--p 00004000 00:2b 250709985                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
7f1633c8a000-7f1633c8b000 r--p 00006000 00:2b 250709985                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
7f1633c8b000-7f1633c8c000 rw-p 00007000 00:2b 250709985                  /sfs/applications/202307/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
7f1633c8c000-7f1633c93000 r--s 00000000 00:24 40748454                   /usr/lib64/gconv/gconv-modules.cache
7f1633c93000-7f1633c94000 rw-p 00000000 00:00 0 
7f1633c94000-7f1633c95000 r--p 00021000 00:24 40383882                   /usr/lib64/ld-2.17.so
7f1633c95000-7f1633c96000 rw-p 00022000 00:24 40383882                   /usr/lib64/ld-2.17.so
7f1633c96000-7f1633c97000 rw-p 00000000 00:00 0 
7ffe3a9b3000-7ffe3a9d6000 rw-p 00000000 00:00 0                          [stack]
7ffe3a9dd000-7ffe3a9df000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
srun: error: udc-aj37-36: tasks 21,23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.10
slurmstepd: error: *** STEP 55131464.10 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:12:11 ***
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-6: Exited with exit code 1
srun: error: udc-aj37-35: tasks 16,19: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9,11: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Aborted (core dumped)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
srun: error: udc-aj37-36: tasks 20,22-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.11
slurmstepd: error: *** STEP 55131464.11 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:13:20 ***
srun: error: udc-aj40-35: tasks 28,30-31: Exited with exit code 1
srun: error: udc-aj37-35: tasks 16-17: Exited with exit code 1
srun: error: udc-aj34-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj38-35: tasks 24,26-27: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-6: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 248, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 90, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_tp_benchmark.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_tp_benchmark.py", line 57, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder, 
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 223, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts, world_size=world_size, rank=rank))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 182, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 123, in __init__
    self.self_attn = ParallelMultiheadAttention(d_model, nhead, dropout=dropout, world_size=world_size, rank=rank)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 543, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.12
slurmstepd: error: *** STEP 55131464.12 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:13:29 ***
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-30: Terminated
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj36-35: tasks 14-15: Terminated
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj37-35: tasks 18-19: Terminated
srun: error: udc-aj37-36: tasks 21-23: Terminated
srun: error: udc-aj38-35: tasks 24-26: Terminated
srun: error: udc-aj33-9: tasks 0-1,3: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj34-35: tasks 9-10: Terminated
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj38-35: task 27: Terminated
srun: error: udc-aj36-35: task 13: Terminated
srun: error: udc-aj33-9: task 2: Terminated
srun: error: udc-aj37-35: task 17: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: Force Terminated StepId=55131464.12
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 248, in benchmark_fsdp
    model = RotatedTensorParallel(model)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.13
slurmstepd: error: *** STEP 55131464.13 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:13:45 ***
srun: error: udc-aj37-36: tasks 20-22: Terminated
srun: error: udc-aj40-35: tasks 28,30-31: Terminated
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj38-35: tasks 24,26-27: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj37-36: task 23: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj40-35: task 29: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj38-35: task 25: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: Force Terminated StepId=55131464.13
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 249, in benchmark_fsdp
    model = RotatedTensorParallel(model, inplace=True)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 441, in __init__
    self.RecursiveVisit('module', self.module, self)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 458, in RecursiveVisit
    self.RecursiveVisit(name, child, module)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 514, in RecursiveVisit
    r = ParallelMultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=bias,
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 42, in __init__
    self.num_heads_per_partition = divide_and_check_no_remainder(self.num_heads, self.world_size)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 69, in divide_and_check_no_remainder
    ensure_divisibility(numerator, denominator)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/utils.py", line 63, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 16 is not divisible by 32
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.14
slurmstepd: error: *** STEP 55131464.14 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:14:02 ***
srun: error: udc-aj37-36: tasks 20-21,23: Terminated
srun: error: udc-aj40-35: task 28: Terminated
srun: error: udc-aj37-35: tasks 18-19: Terminated
srun: error: udc-aj40-35: tasks 30-31: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj37-35: task 17: Terminated
srun: error: udc-aj40-35: task 29: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.14
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 3.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 3.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5443, in multi_head_attention_forward
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 3.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 22-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.15
slurmstepd: error: *** STEP 55131464.15 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:14:51 ***
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj40-35: tasks 28,31: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj38-35: tasks 26-27: Terminated
srun: error: udc-aj36-35: tasks 13-14: Terminated
srun: error: udc-aj34-35: tasks 9-10: Terminated
srun: error: udc-aj33-10: tasks 5-6: Terminated
srun: error: udc-aj33-9: tasks 0-1: Terminated
srun: error: udc-aj38-35: task 25: Terminated
srun: error: udc-aj36-35: task 12: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: error: udc-aj34-35: task 8: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj40-35: task 30: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj33-10: task 4: Terminated
srun: error: udc-aj33-9: task 2: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: Force Terminated StepId=55131464.15
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 186.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 9.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 186.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 9.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 186.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 186.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 4826, in _in_projection_packed
    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.07 GiB is allocated by PyTorch, and 126.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 21,23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.16
slurmstepd: error: *** STEP 55131464.16 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:15:38 ***
srun: error: udc-aj40-35: tasks 28-29: Exited with exit code 1
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj36-35: tasks 14-15: Terminated
srun: error: udc-aj34-35: tasks 8,10: Terminated
srun: error: udc-aj33-9: tasks 1-2: Terminated
srun: error: udc-aj33-10: tasks 6-7: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj36-35: task 12: Terminated
srun: error: udc-aj34-35: task 9: Terminated
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj33-10: task 4: Terminated
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj40-35: task 30: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj36-35: task 13: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj37-35: task 16: Terminated
srun: error: udc-aj33-10: task 5: Terminated
srun: Force Terminated StepId=55131464.16
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 347.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 347.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 347.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 407.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 331.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 307.62 MiB is free. Including non-PyTorch memory, this process has 31.44 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 367.62 MiB is free. Including non-PyTorch memory, this process has 31.38 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 387.62 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 391.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 347.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 367.62 MiB is free. Including non-PyTorch memory, this process has 31.38 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 391.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 391.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 371.62 MiB is free. Including non-PyTorch memory, this process has 31.37 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 347.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 347.62 MiB is free. Including non-PyTorch memory, this process has 31.40 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 327.62 MiB is free. Including non-PyTorch memory, this process has 31.42 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 371.62 MiB is free. Including non-PyTorch memory, this process has 31.37 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 371.62 MiB is free. Including non-PyTorch memory, this process has 31.37 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 351.62 MiB is free. Including non-PyTorch memory, this process has 31.39 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 21-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.17
slurmstepd: error: *** STEP 55131464.17 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:16:13 ***
srun: error: udc-aj40-35: tasks 28,31: Exited with exit code 1
srun: error: udc-aj38-35: tasks 26-27: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Exited with exit code 1
srun: error: udc-aj36-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-6: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj40-35: task 30: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj36-35: tasks 12-13: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.18
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
slurmstepd: error: *** STEP 55131464.18 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:18:19 ***
srun: error: udc-aj33-10: tasks 6-7: Exited with exit code 1
srun: error: udc-aj37-36: tasks 21-23: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-30: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj38-35: tasks 24,26: Exited with exit code 1
srun: error: udc-aj36-35: task 14: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Terminated
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Terminated
srun: Force Terminated StepId=55131464.18
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 21-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.19
slurmstepd: error: *** STEP 55131464.19 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:20:51 ***
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-6: Exited with exit code 1
srun: error: udc-aj34-35: tasks 8-9: Exited with exit code 1
srun: error: udc-aj36-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj33-9: tasks 2-3: Terminated
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Terminated
srun: Force Terminated StepId=55131464.19
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 20,22-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.20
slurmstepd: error: *** STEP 55131464.20 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:22:31 ***
srun: error: udc-aj38-35: tasks 24,26-27: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-19: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-31: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj33-10: tasks 6-7: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1-2: Exited with exit code 1
srun: error: udc-aj34-35: tasks 8-9: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 20-21: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.21
slurmstepd: error: *** STEP 55131464.21 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:23:29 ***
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28-30: Terminated
srun: error: udc-aj38-35: tasks 26-27: Terminated
srun: error: udc-aj38-35: task 25: Terminated
srun: error: udc-aj37-36: task 23: Terminated
srun: error: udc-aj36-35: tasks 13-15: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj36-35: task 12: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.21
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 196, in forward
    return F.layer_norm(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 2543, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 196, in forward
    return F.layer_norm(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 2543, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 196, in forward
    return F.layer_norm(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 2543, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 196, in forward
    return F.layer_norm(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 2543, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 160, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 29.77 GiB memory in use. Of the allocated memory 25.62 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 135.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.87 GiB is allocated by PyTorch, and 239.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: task 30: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.22
srun: error: udc-aj37-36: task 22: Exited with exit code 1
slurmstepd: error: *** STEP 55131464.22 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:24:06 ***
srun: error: udc-aj38-35: task 26: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Exited with exit code 1
srun: error: udc-aj34-35: task 10: Exited with exit code 1
srun: error: udc-aj36-35: task 14: Exited with exit code 1
srun: error: udc-aj33-9: task 2: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 399.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 383.62 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 399.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 399.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 383.62 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 383.62 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 491.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 425.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 173, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 403.62 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 564.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 20-21: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.23
slurmstepd: error: *** STEP 55131464.23 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:26:56 ***
srun: error: udc-aj37-35: tasks 16-17: Exited with exit code 1
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: error: udc-aj34-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Exited with exit code 1
srun: error: udc-aj36-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-30: Terminated
srun: error: udc-aj38-35: tasks 25-26: Terminated
srun: error: udc-aj34-35: task 8: Terminated
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj33-9: task 1: Terminated
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj33-10: tasks 6-7: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj33-9: tasks 0,3: Terminated
srun: error: udc-aj33-10: task 4: Terminated
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Terminated
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Terminated
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Terminated
srun: error: udc-aj33-9: task 2: Terminated
srun: Force Terminated StepId=55131464.23
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 331.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 331.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 487.62 MiB is free. Including non-PyTorch memory, this process has 31.26 GiB memory in use. Of the allocated memory 30.29 GiB is allocated by PyTorch, and 428.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 331.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 335.62 MiB is free. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 315.62 MiB is free. Including non-PyTorch memory, this process has 31.43 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 315.62 MiB is free. Including non-PyTorch memory, this process has 31.43 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 174, in train
    loss.backward()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 315.62 MiB is free. Including non-PyTorch memory, this process has 31.43 GiB memory in use. Of the allocated memory 30.28 GiB is allocated by PyTorch, and 632.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: tasks 29-30: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.24
slurmstepd: error: *** STEP 55131464.24 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:30:06 ***
srun: error: udc-aj37-36: tasks 21-22: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj33-9: tasks 0,3: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj36-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj33-10: tasks 4,7: Exited with exit code 1
srun: error: udc-aj33-9: task 2: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: task 22: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.25
slurmstepd: error: *** STEP 55131464.25 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:31:50 ***
srun: error: udc-aj37-36: task 21: Terminated
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: task 28: Terminated
srun: error: udc-aj40-35: tasks 30-31: Terminated
srun: error: udc-aj37-36: task 23: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj40-35: task 29: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.25
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.26
slurmstepd: error: *** STEP 55131464.26 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:33:37 ***
srun: error: udc-aj37-36: tasks 20,23: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28-30: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj34-35: tasks 8,10-11: Terminated
srun: error: udc-aj36-35: tasks 12-13,15: Terminated
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj36-35: task 14: Terminated
srun: error: udc-aj34-35: task 9: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.26
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 137.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 489.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 101.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 117.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 157.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 489.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 137.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 489.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.78 GiB is allocated by PyTorch, and 449.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 117.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 137.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 489.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.78 GiB is allocated by PyTorch, and 449.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.62 GiB is allocated by PyTorch, and 529.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.27
slurmstepd: error: *** STEP 55131464.27 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:34:16 ***
srun: error: udc-aj37-36: tasks 21-22: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-30: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17,19: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj33-9: tasks 2-3: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5-6: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.28
slurmstepd: error: *** STEP 55131464.28 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:41:13 ***
srun: error: udc-aj37-36: tasks 21-22: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-30: Exited with exit code 1
srun: error: udc-aj33-10: tasks 5,7: Exited with exit code 1
srun: error: udc-aj37-35: tasks 18-19: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25,27: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj38-35: task 26: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-9: tasks 2-3: Terminated
srun: error: udc-aj36-35: tasks 12,14: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Terminated
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Terminated
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: Force Terminated StepId=55131464.28
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 22: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 23: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 21: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 25: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 26: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 29: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 30: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 31: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 17: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 19: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 20: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 24: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 28: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 27: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 18: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 16: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 22-23: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.29
slurmstepd: error: *** STEP 55131464.29 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:49:25 ***
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
srun: error: udc-aj37-35: tasks 17-18: Exited with exit code 1
srun: error: udc-aj37-36: task 21: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj33-10: task 5: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj38-35: tasks 25-26: Exited with exit code 1
srun: error: udc-aj34-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj33-10: task 6: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Terminated
srun: error: udc-aj38-35: task 27: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-9: tasks 0,2: Terminated
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj36-35: tasks 12,14: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj34-35: task 11: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Terminated
srun: Force Terminated StepId=55131464.29
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 999, in __init__
    self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 531628032 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 21: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.30
slurmstepd: error: *** STEP 55131464.30 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:51:56 ***
srun: error: udc-aj37-36: tasks 20,23: Terminated
srun: error: udc-aj40-35: task 31: Killed
srun: error: udc-aj40-35: task 28: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj40-35: task 29: Terminated
srun: error: udc-aj40-35: task 30: Terminated
srun: error: udc-aj38-35: tasks 24-26: Terminated
srun: error: udc-aj37-35: tasks 17-19: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj37-35: task 16: Terminated
srun: error: udc-aj38-35: task 27: Killed
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj33-9: tasks 0,2-3: Terminated
srun: error: udc-aj33-10: tasks 4,6-7: Terminated
srun: error: udc-aj33-9: task 1: Terminated
srun: error: udc-aj33-10: task 5: Terminated
srun: Force Terminated StepId=55131464.30
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1008, in __init__
    self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 128, in __init__
    super().__init__(in_features, out_features, bias=bias,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 177209344 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 21: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.31
slurmstepd: error: *** STEP 55131464.31 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:54:36 ***
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 119, in __init__
    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 999, in __init__
    self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 531628032 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: tasks 20,22: Terminated
srun: error: udc-aj37-36: task 23: Terminated
srun: error: udc-aj40-35: task 30: Killed
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28,31: Terminated
srun: error: udc-aj38-35: tasks 24-25,27: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj34-35: tasks 8,10-11: Terminated
srun: error: udc-aj40-35: task 29: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj33-10: tasks 4-5: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj34-35: task 9: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj33-10: task 6: Terminated
srun: error: udc-aj38-35: task 26: Killed
srun: Force Terminated StepId=55131464.31
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 29.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 29.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 29.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 20-22: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.32
slurmstepd: error: *** STEP 55131464.32 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:55:20 ***
srun: error: udc-aj40-35: tasks 30-31: Exited with exit code 1
srun: error: udc-aj38-35: tasks 26-27: Exited with exit code 1
srun: error: udc-aj37-35: tasks 18-19: Exited with exit code 1
srun: error: udc-aj36-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj34-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj33-9: tasks 1,3: Exited with exit code 1
srun: error: udc-aj40-35: task 29: Exited with exit code 1
srun: error: udc-aj38-35: task 24: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj33-10: tasks 6-7: Terminated
srun: error: udc-aj33-10: task 5: Terminated
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-9: task 2: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Terminated
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: Force Terminated StepId=55131464.32
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 22: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.33
slurmstepd: error: *** STEP 55131464.33 ON udc-aj33-9 CANCELLED AT 2023-11-19T18:57:51 ***
srun: error: udc-aj37-36: task 21: Terminated
srun: error: udc-aj37-36: task 23: Terminated
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj38-35: task 26: Killed
srun: error: udc-aj38-35: tasks 25,27: Terminated
srun: error: udc-aj40-35: tasks 28,30: Terminated
srun: error: udc-aj40-35: task 29: Killed
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.33
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 23: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.34
slurmstepd: error: *** STEP 55131464.34 ON udc-aj33-9 CANCELLED AT 2023-11-19T19:00:30 ***
srun: error: udc-aj37-36: task 21: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28,30-31: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj34-35: tasks 9-11: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 8: Terminated
srun: error: udc-aj40-35: task 29: Killed
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.34
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 21: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.35
slurmstepd: error: *** STEP 55131464.35 ON udc-aj33-9 CANCELLED AT 2023-11-19T19:03:07 ***
srun: error: udc-aj37-36: task 23: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28,30-31: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj40-35: task 29: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.35
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 21: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.36
slurmstepd: error: *** STEP 55131464.36 ON udc-aj33-9 CANCELLED AT 2023-11-19T19:05:45 ***
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28-30: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj34-35: tasks 8,10-11: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj33-10: tasks 4,6-7: Terminated
srun: error: udc-aj40-35: task 31: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 9: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj33-10: task 5: Terminated
srun: Force Terminated StepId=55131464.36
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 117.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 117.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 101.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 101.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 101.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 117.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 97.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.93 GiB is allocated by PyTorch, and 218.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj34-35: tasks 10-11: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131464.37
slurmstepd: error: *** STEP 55131464.37 ON udc-aj33-9 CANCELLED AT 2023-11-19T19:06:34 ***
srun: error: udc-aj40-35: task 28: Exited with exit code 1
srun: error: udc-aj37-35: task 19: Exited with exit code 1
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj36-35: task 15: Exited with exit code 1
srun: error: udc-aj33-9: task 1: Exited with exit code 1
srun: error: udc-aj38-35: tasks 24,27: Exited with exit code 1
srun: error: udc-aj33-10: task 7: Exited with exit code 1
srun: error: udc-aj34-35: task 9: Exited with exit code 1
srun: error: udc-aj40-35: tasks 29-30: Exited with exit code 1
srun: error: udc-aj37-35: task 17: Exited with exit code 1
srun: error: udc-aj37-35: task 18: Terminated
srun: error: udc-aj37-36: tasks 21-22: Exited with exit code 1
srun: error: udc-aj38-35: task 26: Exited with exit code 1
srun: error: udc-aj36-35: task 13: Terminated
srun: error: udc-aj36-35: task 14: Exited with exit code 1
srun: error: udc-aj33-9: task 0: Exited with exit code 1
srun: error: udc-aj33-9: task 2: Terminated
srun: error: udc-aj33-10: tasks 5-6: Terminated
srun: error: udc-aj40-35: task 31: Exited with exit code 1
srun: error: udc-aj37-35: task 16: Exited with exit code 1
srun: error: udc-aj34-35: task 8: Exited with exit code 1
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj36-35: task 12: Exited with exit code 1
srun: error: udc-aj33-10: task 4: Terminated
srun: error: udc-aj37-36: task 20: Exited with exit code 1
srun: error: udc-aj38-35: task 25: Exited with exit code 1
srun: Force Terminated StepId=55131464.37
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 22: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.38
slurmstepd: error: *** STEP 55131464.38 ON udc-aj33-9 CANCELLED AT 2023-11-19T19:09:05 ***
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-36: task 21: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28-30: Terminated
srun: error: udc-aj38-35: tasks 24,26-27: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj34-35: tasks 8,10-11: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj40-35: task 31: Killed
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj34-35: task 9: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: error: udc-aj38-35: task 25: Killed
srun: Force Terminated StepId=55131464.38
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 21: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131464.39
slurmstepd: error: *** STEP 55131464.39 ON udc-aj33-9 CANCELLED AT 2023-11-19T19:11:43 ***
srun: error: udc-aj37-36: task 23: Exited with exit code 1
srun: error: udc-aj37-36: task 20: Terminated
srun: error: udc-aj37-36: task 22: Terminated
srun: error: udc-aj37-35: tasks 16-18: Terminated
srun: error: udc-aj40-35: tasks 28-29,31: Terminated
srun: error: udc-aj38-35: tasks 25-27: Terminated
srun: error: udc-aj37-35: task 19: Terminated
srun: error: udc-aj33-9: tasks 0-2: Terminated
srun: error: udc-aj34-35: tasks 8-10: Terminated
srun: error: udc-aj36-35: tasks 12-14: Terminated
srun: error: udc-aj33-10: tasks 4-6: Terminated
srun: error: udc-aj40-35: task 30: Killed
srun: error: udc-aj33-9: task 3: Terminated
srun: error: udc-aj34-35: task 11: Terminated
srun: error: udc-aj38-35: task 24: Terminated
srun: error: udc-aj36-35: task 15: Terminated
srun: error: udc-aj33-10: task 7: Terminated
srun: Force Terminated StepId=55131464.39
