Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
srun: error: udc-aj37-36: tasks 5,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.0
slurmstepd: error: *** STEP 55131465.0 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:13:04 ***
srun: error: udc-aj40-35: tasks 13,15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 14: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: Force Terminated StepId=55131465.0
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
srun: error: udc-aj37-36: tasks 4,6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.1
slurmstepd: error: *** STEP 55131465.1 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:13:44 ***
srun: error: udc-aj40-35: tasks 13-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Terminated
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: Force Terminated StepId=55131465.1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 15.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 758.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 758.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 131.62 MiB is free. Including non-PyTorch memory, this process has 31.61 GiB memory in use. Of the allocated memory 30.47 GiB is allocated by PyTorch, and 654.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 171.62 MiB is free. Including non-PyTorch memory, this process has 31.57 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 598.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 171.62 MiB is free. Including non-PyTorch memory, this process has 31.57 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 619.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 758.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.47 GiB is allocated by PyTorch, and 776.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 738.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 15.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.47 GiB is allocated by PyTorch, and 770.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.47 GiB is allocated by PyTorch, and 774.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 155.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.47 GiB is allocated by PyTorch, and 630.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 151.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 619.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 155.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 643.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 13.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.68 GiB is allocated by PyTorch, and 560.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 151.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 621.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 212, in forward
    return self.linear(src)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.67 GiB is allocated by PyTorch, and 581.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.2
slurmstepd: error: *** STEP 55131465.2 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:14:15 ***
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 8-9,11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 10: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.3
slurmstepd: error: *** STEP 55131465.3 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:15:06 ***
srun: error: udc-aj37-36: tasks 5-6: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 2-3: Terminated
srun: error: udc-aj37-35: task 1: Terminated
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-36: task 7: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Terminated
srun: Force Terminated StepId=55131465.3
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.4
slurmstepd: error: *** STEP 55131465.4 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:16:03 ***
srun: error: udc-aj40-35: task 14: Exited with exit code 1
srun: error: udc-aj37-36: tasks 5,7: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj40-35: tasks 12,15: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 0,2: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Terminated
srun: Force Terminated StepId=55131465.4
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 21.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 109, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 17.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 154, in step
    self._init_group(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 11.62 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.20 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.5
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
slurmstepd: error: *** STEP 55131465.5 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:16:42 ***
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
srun: error: udc-aj37-36: tasks 4-6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.6
slurmstepd: error: *** STEP 55131465.6 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:17:47 ***
srun: error: udc-aj40-35: tasks 13-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Exited with exit code 1
srun: error: udc-aj37-36: task 7: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 163.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 147.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 147.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 163.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 147.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 163.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 167.62 MiB is free. Including non-PyTorch memory, this process has 31.57 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 143.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1471, in relu
    result = torch.relu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 167.62 MiB is free. Including non-PyTorch memory, this process has 31.57 GiB memory in use. Of the allocated memory 29.51 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.7
slurmstepd: error: *** STEP 55131465.7 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:18:19 ***
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1,3: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj37-35: task 2: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.8
slurmstepd: error: *** STEP 55131465.8 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:19:30 ***
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 8-9: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 0-1: Terminated
srun: error: udc-aj37-35: task 2: Terminated
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 10: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Terminated
srun: Force Terminated StepId=55131465.8
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.9
slurmstepd: error: *** STEP 55131465.9 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:20:50 ***
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: tasks 2-3: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Terminated
srun: Force Terminated StepId=55131465.9
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.16 GiB is allocated by PyTorch, and 77.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.11 GiB is allocated by PyTorch, and 77.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 177, in train
    optimizer.step()
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.17 GiB is allocated by PyTorch, and 76.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 4-5,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.10
slurmstepd: error: *** STEP 55131465.10 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:21:31 ***
srun: error: udc-aj40-35: tasks 12,14-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 228, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
srun: error: udc-aj37-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.11
slurmstepd: error: *** STEP 55131465.11 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:22:34 ***
srun: error: udc-aj40-35: tasks 12,14-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 8,10-11: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 119.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 523.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 139.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 523.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 183.62 MiB is free. Including non-PyTorch memory, this process has 31.56 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 459.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 203.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 459.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 183.62 MiB is free. Including non-PyTorch memory, this process has 31.56 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 459.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 119.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 523.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 123.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 523.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 183.62 MiB is free. Including non-PyTorch memory, this process has 31.56 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 459.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 203.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 459.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 187.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 459.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 55.62 MiB is free. Including non-PyTorch memory, this process has 31.68 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 587.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 123.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 523.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 247.62 MiB is free. Including non-PyTorch memory, this process has 31.49 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 395.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 271.62 MiB is free. Including non-PyTorch memory, this process has 31.47 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 395.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 247.62 MiB is free. Including non-PyTorch memory, this process has 31.49 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 395.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 271.62 MiB is free. Including non-PyTorch memory, this process has 31.47 GiB memory in use. Of the allocated memory 30.61 GiB is allocated by PyTorch, and 395.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.12
slurmstepd: error: *** STEP 55131465.12 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:23:06 ***
srun: error: udc-aj40-35: tasks 12-14: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 4-5,7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.13
slurmstepd: error: *** STEP 55131465.13 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:24:10 ***
srun: error: udc-aj40-35: tasks 12-14: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-3: Terminated
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Terminated
srun: Force Terminated StepId=55131465.13
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.14
slurmstepd: error: *** STEP 55131465.14 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:25:25 ***
srun: error: udc-aj40-35: tasks 13-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Exited with exit code 1
srun: error: udc-aj37-35: tasks 0,2-3: Terminated
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Terminated
srun: Force Terminated StepId=55131465.14
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 3.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 3.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 5.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.15
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_dp_benchmark.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
srun: error: udc-aj37-35: task 2: Exited with exit code 1
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 1.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 55.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
slurmstepd: error: *** STEP 55131465.15 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:26:10 ***
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj37-35: tasks 0-1: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: tasks 10-11: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Terminated
srun: Force Terminated StepId=55131465.15
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 467.62 MiB is free. Including non-PyTorch memory, this process has 31.28 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 451.62 MiB is free. Including non-PyTorch memory, this process has 31.29 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 451.62 MiB is free. Including non-PyTorch memory, this process has 31.29 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 471.62 MiB is free. Including non-PyTorch memory, this process has 31.28 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 467.62 MiB is free. Including non-PyTorch memory, this process has 31.28 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 471.62 MiB is free. Including non-PyTorch memory, this process has 31.28 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 253, in benchmark_fsdp
    benchmark_language_model(model_config, rfsdp_model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 218, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_fsdp_benchmark.py", line 173, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/functional.py", line 5440, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 447.62 MiB is free. Including non-PyTorch memory, this process has 31.30 GiB memory in use. Of the allocated memory 30.64 GiB is allocated by PyTorch, and 161.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.16
slurmstepd: error: *** STEP 55131465.16 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:26:53 ***
srun: error: udc-aj37-35: task 2: Exited with exit code 1
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Terminated
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.16
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 30.57 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 30.57 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 30.59 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 30.57 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.24 GiB is free. Including non-PyTorch memory, this process has 30.49 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 30.52 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.24 GiB is free. Including non-PyTorch memory, this process has 30.49 GiB memory in use. Of the allocated memory 28.42 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 4,6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.17
slurmstepd: error: *** STEP 55131465.17 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:27:20 ***
srun: error: udc-aj40-35: tasks 12-13,15: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9,11: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj40-35: task 14: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj38-35: task 10: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 225, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.18
slurmstepd: error: *** STEP 55131465.18 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:29:03 ***
srun: error: udc-aj37-36: tasks 5,7: Exited with exit code 1
srun: error: udc-aj40-35: tasks 12-13: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj40-35: task 14: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Terminated
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: tasks 0,3: Terminated
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj37-35: task 2: Terminated
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: Force Terminated StepId=55131465.18
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 7: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 6: did you call init?
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 9: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 8: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 5: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 4: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 10: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 11: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 12: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 13: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 14: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 226, in benchmark_language_model
    dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())["allocated_bytes.all.peak"] / 2**30
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 256, in memory_stats
    stats = memory_stats_as_nested_dict(device=device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/cuda/memory.py", line 268, in memory_stats_as_nested_dict
    return torch._C._cuda_memoryStats(device)
RuntimeError: Invalid device argument 15: did you call init?
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.19
slurmstepd: error: *** STEP 55131465.19 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:31:11 ***
srun: error: udc-aj40-35: tasks 12-13: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj40-35: task 14: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1,3: Terminated
srun: error: udc-aj37-35: task 2: Terminated
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Terminated
srun: Force Terminated StepId=55131465.19
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.26 GiB is free. Including non-PyTorch memory, this process has 25.48 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 2 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 3 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 249, in benchmark_fsdp
    rfsdp_model = DDP(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 809, in __init__
    self._ddp_init_helper(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1098, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.24 GiB is free. Including non-PyTorch memory, this process has 25.50 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 5-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.20
slurmstepd: error: *** STEP 55131465.20 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:32:37 ***
srun: error: udc-aj37-35: tasks 0,2: Exited with exit code 1
srun: error: udc-aj40-35: tasks 13-15: Exited with exit code 1
srun: error: udc-aj38-35: tasks 9-11: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 1 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 251, in benchmark_fsdp
    rfsdp_model = FullyShardedDataParallel(model, **config)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 687, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 800, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 792, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.98 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 25.36 GiB memory in use. Of the allocated memory 25.06 GiB is allocated by PyTorch, and 3.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 4-5: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.21
slurmstepd: error: *** STEP 55131465.21 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:33:34 ***
srun: error: udc-aj37-36: task 6: Terminated
srun: error: udc-aj37-35: tasks 0-2: Terminated
srun: error: udc-aj40-35: tasks 12,14-15: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj40-35: task 13: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.21
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 29.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 93.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 121.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 89.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 121.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 157.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 57.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 25.62 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 153.62 MiB is free. Including non-PyTorch memory, this process has 31.59 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 57.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 89.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 121.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 89.62 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 121.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 49.62 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 185.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.22
slurmstepd: error: *** STEP 55131465.22 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:34:05 ***
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 113.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 57.62 MiB is free. Including non-PyTorch memory, this process has 31.68 GiB memory in use. Of the allocated memory 30.89 GiB is allocated by PyTorch, and 281.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 121.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.76 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 830, in multi_head_attention_forward
    attn_output = torch._C._nn.linear(attn_output, out_proj_weight, out_proj_bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 57.62 MiB is free. Including non-PyTorch memory, this process has 31.68 GiB memory in use. Of the allocated memory 30.89 GiB is allocated by PyTorch, and 281.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-35: task 2: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.23
slurmstepd: error: *** STEP 55131465.23 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:35:31 ***
srun: error: udc-aj37-36: tasks 5-6: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Exited with exit code 1
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj37-36: task 7: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj40-35: task 12: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.23
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 73.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 109.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 93.62 MiB is free. Including non-PyTorch memory, this process has 31.64 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 105.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 105.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 73.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 65.62 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 73.62 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 156, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 163, in _sa_block
    x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 400, in forward
    outputs = self.module_list[index](*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/attention.py", line 131, in forward
    attn_output, attn_output_weights = multi_head_attention_forward(
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/functional.py", line 827, in multi_head_attention_forward
    attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 105.62 MiB is free. Including non-PyTorch memory, this process has 31.63 GiB memory in use. Of the allocated memory 30.90 GiB is allocated by PyTorch, and 253.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-35: task 1: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.24
slurmstepd: error: *** STEP 55131465.24 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:36:59 ***
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: error: udc-aj37-36: tasks 4-5: Exited with exit code 1
srun: error: udc-aj37-35: tasks 2-3: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj37-36: task 7: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.25
slurmstepd: error: *** STEP 55131465.25 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:38:41 ***
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-36: task 6: Terminated
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-35: tasks 1-3: Terminated
srun: error: udc-aj40-35: task 12: Terminated
srun: error: udc-aj40-35: task 14: Terminated
srun: error: udc-aj40-35: task 13: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.25
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 31.28 GiB is allocated by PyTorch, and 1.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.26
slurmstepd: error: *** STEP 55131465.26 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:40:29 ***
srun: error: udc-aj37-36: tasks 6-7: Terminated
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-35: tasks 1-3: Terminated
srun: error: udc-aj40-35: tasks 12,14: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj40-35: task 13: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.26
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 1 has a total capacty of 31.74 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 29.59 GiB memory in use. Of the allocated memory 25.42 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 2 has a total capacty of 31.74 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 29.59 GiB memory in use. Of the allocated memory 25.42 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacty of 31.74 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 29.57 GiB memory in use. Of the allocated memory 25.42 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 3 has a total capacty of 31.74 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 29.58 GiB memory in use. Of the allocated memory 25.42 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.27
slurmstepd: error: *** STEP 55131465.27 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:41:04 ***
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-36: tasks 5-6: Terminated
srun: error: udc-aj37-35: tasks 1,3: Terminated
srun: error: udc-aj38-35: tasks 8,10: Terminated
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj38-35: task 9: Terminated
srun: error: udc-aj37-35: task 2: Terminated
srun: Force Terminated StepId=55131465.27
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 157.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 127.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 123.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 133.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 125.62 MiB is free. Including non-PyTorch memory, this process has 31.61 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 127.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 251, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 215, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark.py", line 170, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 155.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 119.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.28
slurmstepd: error: *** STEP 55131465.28 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:43:47 ***
srun: error: udc-aj37-36: tasks 5,7: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Exited with exit code 1
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: error: udc-aj37-35: tasks 2-3: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: Force Terminated StepId=55131465.28
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 135.62 MiB is free. Including non-PyTorch memory, this process has 31.60 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 118.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 252, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 216, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 171, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 542, in forward
    outputs = self.module(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 195, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 157, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 171, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 72, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/rotated_tensor_parallel.py", line 420, in forward
    output_parallel = torch.cat(output_list, dim=self.output_partition_dim).contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 159.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 30.97 GiB is allocated by PyTorch, and 91.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 5-6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.29
slurmstepd: error: *** STEP 55131465.29 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:46:25 ***
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj40-35: task 12: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.29
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 5: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.30
slurmstepd: error: *** STEP 55131465.30 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:48:55 ***
srun: error: udc-aj37-36: tasks 4,7: Terminated
srun: error: udc-aj37-36: task 6: Terminated
srun: error: udc-aj37-35: tasks 0-2: Terminated
srun: error: udc-aj40-35: tasks 12,14-15: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj40-35: task 13: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.30
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 5: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.31
slurmstepd: error: *** STEP 55131465.31 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:51:33 ***
srun: error: udc-aj37-36: tasks 4,6: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-35: tasks 0,2-3: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj40-35: tasks 12,14: Terminated
srun: error: udc-aj37-35: task 1: Terminated
srun: error: udc-aj38-35: tasks 8-9,11: Terminated
srun: error: udc-aj40-35: task 13: Terminated
srun: error: udc-aj38-35: task 10: Killed
srun: Force Terminated StepId=55131465.31
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 30.56 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 1.17 GiB is free. Including non-PyTorch memory, this process has 30.56 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 30.56 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 30.54 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 161.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 1.17 GiB is free. Including non-PyTorch memory, this process has 30.56 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 181.62 MiB is free. Including non-PyTorch memory, this process has 31.56 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 161.62 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 389.62 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 369.62 MiB is free. Including non-PyTorch memory, this process has 31.38 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 369.62 MiB is free. Including non-PyTorch memory, this process has 31.38 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 373.62 MiB is free. Including non-PyTorch memory, this process has 31.37 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 0 has a total capacty of 31.74 GiB of which 393.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 2 has a total capacty of 31.74 GiB of which 369.62 MiB is free. Including non-PyTorch memory, this process has 31.38 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 1 has a total capacty of 31.74 GiB of which 369.62 MiB is free. Including non-PyTorch memory, this process has 31.38 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 490, in _gather
    torch.distributed.all_gather(tensor_list, input_, group=group)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2808, in all_gather
    work = group.allgather([tensor_list], [tensor])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 3 has a total capacty of 31.74 GiB of which 393.62 MiB is free. Including non-PyTorch memory, this process has 31.35 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj40-35: tasks 14-15: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.32
slurmstepd: error: *** STEP 55131465.32 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:52:18 ***
srun: error: udc-aj38-35: tasks 10-11: Exited with exit code 1
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: error: udc-aj37-35: tasks 1-2: Exited with exit code 1
srun: error: udc-aj40-35: task 13: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj37-35: task 3: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj38-35: task 9: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 6: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.33
slurmstepd: error: *** STEP 55131465.33 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:54:49 ***
srun: error: udc-aj37-36: task 5: Terminated
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-35: tasks 0-2: Terminated
srun: error: udc-aj40-35: tasks 12-14: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.33
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 708837376 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 6: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.34
slurmstepd: error: *** STEP 55131465.34 ON udc-aj37-35 CANCELLED AT 2023-11-19T19:57:25 ***
srun: error: udc-aj37-36: tasks 4-5: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-35: tasks 1-3: Terminated
srun: error: udc-aj40-35: tasks 12-13,15: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj40-35: task 14: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.34
Traceback (most recent call last):
  File "benchmarks/multi_dp_benchmark.py", line 271, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_dp_benchmark.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_dp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_dp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 5: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.35
srun: error: udc-aj37-36: task 7: Exited with exit code 1
slurmstepd: error: *** STEP 55131465.35 ON udc-aj37-35 CANCELLED AT 2023-11-19T20:00:03 ***
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-36: task 6: Terminated
srun: error: udc-aj37-35: tasks 0-1,3: Terminated
srun: error: udc-aj40-35: task 12: Terminated
srun: error: udc-aj40-35: tasks 13-14: Terminated
srun: error: udc-aj37-35: task 2: Terminated
srun: error: udc-aj38-35: tasks 8-9,11: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj38-35: task 10: Killed
srun: Force Terminated StepId=55131465.35
Traceback (most recent call last):
  File "benchmarks/multi_fsdp_benchmark.py", line 273, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_fsdp_benchmark.py", line 243, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 88, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_fsdp_benchmark.py", line 34, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_fsdp_benchmark.py", line 56, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder).to(device)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_fsdp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.36
slurmstepd: error: *** STEP 55131465.36 ON udc-aj37-35 CANCELLED AT 2023-11-19T20:02:42 ***
srun: error: udc-aj37-36: task 5: Killed
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-35: tasks 0-2: Terminated
srun: error: udc-aj40-35: tasks 12-14: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj40-35: task 15: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.36
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 197.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 213.62 MiB is free. Including non-PyTorch memory, this process has 31.53 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 197.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 197.62 MiB is free. Including non-PyTorch memory, this process has 31.54 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 213.62 MiB is free. Including non-PyTorch memory, this process has 31.53 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 213.62 MiB is free. Including non-PyTorch memory, this process has 31.53 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 193.62 MiB is free. Including non-PyTorch memory, this process has 31.55 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 217.62 MiB is free. Including non-PyTorch memory, this process has 31.52 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "benchmarks/multi_tp_benchmark.py", line 278, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_tp_benchmark.py", line 256, in benchmark_fsdp
    benchmark_language_model(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 222, in benchmark_language_model
    wps, t = train(model_config, model, benchmark_config, model_specs, args)
  File "benchmarks/multi_tp_benchmark.py", line 177, in train
    output = model(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 199, in forward
    return super().forward(src, self.src_mask)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 161, in forward
    x = self.norm2(x + self._ff_block(x))
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 175, in _ff_block
    return self.ff_block(x)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/models/transformer_lm_tp.py", line 74, in forward
    return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/layers.py", line 197, in forward
    output = gather_from_model_parallel_region(output_parallel)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 581, in gather_from_model_parallel_region
    return _GatherFromModelParallelRegion.apply(input_)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 549, in forward
    return _gather(input_)
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in _gather
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
  File "/sfs/weka/scratch/fad3ew/rtp/rtp/module/collectives.py", line 488, in <listcomp>
    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 217.62 MiB is free. Including non-PyTorch memory, this process has 31.52 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 62.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: udc-aj37-36: tasks 6-7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=55131465.37
slurmstepd: error: *** STEP 55131465.37 ON udc-aj37-35 CANCELLED AT 2023-11-19T20:03:35 ***
srun: error: udc-aj38-35: tasks 9-10: Exited with exit code 1
srun: error: udc-aj40-35: tasks 13-14: Exited with exit code 1
srun: error: udc-aj37-36: task 5: Exited with exit code 1
srun: error: udc-aj37-35: tasks 2-3: Exited with exit code 1
srun: error: udc-aj40-35: task 12: Exited with exit code 1
srun: error: udc-aj38-35: task 8: Exited with exit code 1
srun: error: udc-aj37-35: task 0: Exited with exit code 1
srun: error: udc-aj40-35: task 15: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Exited with exit code 1
srun: error: udc-aj37-35: task 1: Exited with exit code 1
srun: error: udc-aj38-35: task 11: Exited with exit code 1
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark.py", line 240, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 86, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark.py", line 32, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark.py", line 54, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 5: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.38
slurmstepd: error: *** STEP 55131465.38 ON udc-aj37-35 CANCELLED AT 2023-11-19T20:06:05 ***
srun: error: udc-aj37-36: task 6: Exited with exit code 1
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-35: tasks 0-2: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj37-35: task 3: Terminated
srun: error: udc-aj40-35: task 13: Killed
srun: error: udc-aj38-35: task 8: Terminated
srun: error: udc-aj40-35: task 12: Terminated
srun: error: udc-aj40-35: task 14: Killed
srun: error: udc-aj40-35: task 15: Terminated
srun: Force Terminated StepId=55131465.38
Traceback (most recent call last):
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 272, in <module>
    benchmark_fsdp(rank, local_rank, args, world_size)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 241, in benchmark_fsdp
    model_config = create_model_config(args, benchmark_config=benchmark_config, model_specs=model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 87, in create_model_config
    model, optimizer = get_model_and_optimizer(args, device, benchmark_config, model_specs)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 33, in get_model_and_optimizer
    model = get_lm_model(args, device, model_config)
  File "benchmarks/multi_rtp_benchmark_inplace.py", line 55, in get_lm_model
    return transformer_lm.TransformerLM(vocab_size, ninp, nhead, nhid, dropout, initrange, ndecoder)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 216, in __init__
    layers.append(TransformerDecoderLayer(ninp, nhead, nhid, dropout, is_moe, num_local_experts))
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 178, in __init__
    super().__init__(ninp, nhead, nhid, dropout, is_moe=is_moe, num_local_experts=num_local_experts)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 135, in __init__
    self.ff_block = FeedForwardLayer(d_model, dim_feedforward, activation, dropout)
  File "/sfs/weka/scratch/fad3ew/rtp/benchmarks/transformer_lm_dp.py", line 68, in __init__
    self.linear2 = nn.Linear(dim_feedforward, d_model)
  File "/scratch/fad3ew/rtp/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1073741824 bytes. Error code 12 (Cannot allocate memory)
srun: error: udc-aj37-36: task 6: Killed
srun: launch/slurm: _step_signal: Terminating StepId=55131465.39
slurmstepd: error: *** STEP 55131465.39 ON udc-aj37-35 CANCELLED AT 2023-11-19T20:08:42 ***
srun: error: udc-aj37-36: task 7: Terminated
srun: error: udc-aj37-36: task 4: Terminated
srun: error: udc-aj37-36: task 5: Terminated
srun: error: udc-aj37-35: tasks 1-3: Terminated
srun: error: udc-aj40-35: tasks 12-13,15: Terminated
srun: error: udc-aj37-35: task 0: Terminated
srun: error: udc-aj38-35: tasks 9-11: Terminated
srun: error: udc-aj40-35: task 14: Terminated
srun: error: udc-aj38-35: task 8: Terminated
srun: Force Terminated StepId=55131465.39
