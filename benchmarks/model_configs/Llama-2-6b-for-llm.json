{
    "name": "Llama-2-6b-for-llm",
    "num_layers": 28,
    "n_head": 32,
    "hidden_dim": 4096,
    "vocab_size": 32000,
    "max_seq_len": 4096,
    "num_key_value_heads": 32,
    "ffn_embed_dim": 16384,
    "model_type": "llama"
}